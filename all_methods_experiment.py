#!/usr/bin/env python3
"""
BERTæƒ…æ„Ÿåˆ†æžå®žéªŒ - è¿è¡Œæ‰€æœ‰æ•°æ®è¯„ä¼°æ–¹æ³•ï¼ˆé™¤TIMå¤–ï¼‰

ä½¿ç”¨OpenDataValçš„æ‰€æœ‰æ•°æ®è¯„ä¼°æ–¹æ³•å¯¹BERTæƒ…æ„Ÿåˆ†æžè¿›è¡Œæ•°æ®ä»·å€¼è¯„ä¼°å®žéªŒã€‚
å®žéªŒé…ç½®ï¼šè®­ç»ƒé›†2048ï¼Œæµ‹è¯•é›†256ï¼Œç§å­42ï¼Œ30%æ ‡ç­¾ç¿»è½¬
"""

import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Any
import json
import numpy as np
import matplotlib.pyplot as plt

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from opendataval.dataloader import DataFetcher
from opendataval.model import BertClassifier
from opendataval.dataval import (
    LeaveOneOut,
    RandomEvaluator,
    LossValEvaluator
)


def create_bert_model(num_classes=2, device="auto"):
    """åˆ›å»ºBERTåˆ†ç±»å™¨"""
    return BertClassifier(
        pretrained_model_name="distilbert-base-uncased",
        num_classes=num_classes,
        dropout_rate=0.1,
        num_train_layers=2
    )


def prepare_data(train_count=2048, test_count=256, noise_rate=0.3, random_state=42):
    """å‡†å¤‡æ•°æ®é›†"""
    print("ðŸ”„ å‡†å¤‡æ•°æ®é›†...")
    
    # åŠ è½½IMDBæ•°æ®é›† - ä½¿ç”¨é“¾å¼è°ƒç”¨æ–¹å¼
    from opendataval.dataloader import mix_labels
    
    data_fetcher = (
        DataFetcher("imdb", cache_dir="../data_files/", force_download=False)
        .split_dataset_by_count(train_count, test_count, test_count)
    )
    
    print(f"ðŸ“Š æ•°æ®è§„æ¨¡: è®­ç»ƒ={train_count}, éªŒè¯={test_count}, æµ‹è¯•={test_count}")
    
    # æ³¨å…¥æ ‡ç­¾å™ªå£°
    if noise_rate > 0:
        print(f"ðŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£°: {noise_rate*100:.1f}%")
        data_fetcher = data_fetcher.noisify(mix_labels, noise_rate=noise_rate)
    
    return data_fetcher


def get_evaluator_configs():
    """èŽ·å–å¿«é€Ÿæ•°æ®è¯„ä¼°æ–¹æ³•é…ç½®ï¼ˆé™¤TIMå’ŒShapleyå¤–ï¼‰"""
    evaluators_config = {
        # ç®€å•ä¸”å¿«é€Ÿçš„æ–¹æ³•
        "RandomEvaluator": {
            "class": RandomEvaluator,
            "kwargs": {},
            "description": "Random baseline evaluator"
        },
        
        "LeaveOneOut": {
            "class": LeaveOneOut,
            "kwargs": {},
            "description": "Leave-One-Out evaluation"
        },
        
        "LossValEvaluator": {
            "class": LossValEvaluator,
            "kwargs": {
                "is_classification": True,  # æ·»åŠ å¿…éœ€çš„å‚æ•°
            },
            "description": "Loss Value evaluator"
        },
    }
    
    return evaluators_config


def run_evaluator(evaluator_name, evaluator_config, data_fetcher, model, metric="accuracy"):
    """è¿è¡Œå•ä¸ªè¯„ä¼°å™¨"""
    print(f"\nðŸ§ª è¿è¡Œè¯„ä¼°å™¨: {evaluator_name}")
    print(f"   æè¿°: {evaluator_config['description']}")
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨å®žä¾‹
        evaluator_class = evaluator_config["class"]
        evaluator_kwargs = evaluator_config.get("kwargs", {})
        
        evaluator = evaluator_class(**evaluator_kwargs)
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # è®­ç»ƒè¯„ä¼°å™¨
        print(f"   âš¡ å¼€å§‹è®­ç»ƒ...")
        # ä¿®æ­£metricå‚æ•° - ä½¿ç”¨å‡½æ•°è€Œä¸æ˜¯å­—ç¬¦ä¸²
        from sklearn.metrics import accuracy_score
        eval_metric = accuracy_score if metric == "accuracy" else metric
        evaluator_instance = evaluator.train(data_fetcher, model, metric=eval_metric)
        
        # è¯„ä¼°æ•°æ®å€¼
        print(f"   ðŸ“Š è®¡ç®—æ•°æ®å€¼...")
        data_values = evaluator_instance.evaluate_data_values()
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ç»Ÿè®¡ç»“æžœ
        mean_value = np.mean(data_values)
        std_value = np.std(data_values)
        min_value = np.min(data_values)
        max_value = np.max(data_values)
        
        result = {
            "evaluator": evaluator_name,
            "description": evaluator_config['description'],
            "status": "success",
            "duration_seconds": duration,
            "data_values_stats": {
                "mean": float(mean_value),
                "std": float(std_value),
                "min": float(min_value),
                "max": float(max_value),
                "shape": data_values.shape
            },
            "data_values": data_values.tolist(),  # ä¿å­˜å®Œæ•´çš„æ•°å€¼
        }
        
        print(f"   âœ… å®Œæˆ! è€—æ—¶: {duration:.1f}ç§’")
        print(f"   ðŸ“ˆ æ•°æ®å€¼ç»Ÿè®¡: å‡å€¼={mean_value:.4f}, æ ‡å‡†å·®={std_value:.4f}")
        
        return result
        
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {str(e)}")
        print(f"   ðŸ” é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        return {
            "evaluator": evaluator_name,
            "description": evaluator_config['description'],
            "status": "failed",
            "error": str(e),
            "traceback": traceback.format_exc()
        }


def select_high_value_samples(data_values, select_ratio=0.9):
    """é€‰æ‹©é«˜ä»·å€¼æ ·æœ¬ï¼ˆ90%ï¼‰"""
    n_samples = len(data_values)
    n_select = int(n_samples * select_ratio)
    
    # èŽ·å–æ•°æ®å€¼çš„æŽ’åºç´¢å¼•ï¼ˆé™åºï¼‰
    sorted_indices = np.argsort(data_values)[::-1]
    
    # é€‰æ‹©å‰90%çš„é«˜ä»·å€¼æ ·æœ¬
    selected_indices = sorted_indices[:n_select]
    
    return selected_indices


def retrain_with_selected_data(data_fetcher, selected_indices, model_class, metric="accuracy"):
    """ä½¿ç”¨é€‰ä¸­çš„æ•°æ®é‡æ–°è®­ç»ƒæ¨¡åž‹"""
    # åˆ›å»ºæ–°çš„æ•°æ®èŽ·å–å™¨ï¼ŒåªåŒ…å«é€‰ä¸­çš„æ ·æœ¬
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®žé™…çš„DataFetcher APIè¿›è¡Œè°ƒæ•´
    
    # èŽ·å–è®­ç»ƒæ•°æ®
    x_train, y_train = data_fetcher.x_train, data_fetcher.y_train
    x_valid, y_valid = data_fetcher.x_valid, data_fetcher.y_valid
    
    # é€‰æ‹©é«˜ä»·å€¼æ ·æœ¬
    x_train_selected = x_train[selected_indices]
    y_train_selected = y_train[selected_indices]
    
    # åˆ›å»ºæ–°æ¨¡åž‹
    model = model_class()
    
    # è®­ç»ƒæ¨¡åž‹ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®žé™…çš„BERTæ¨¡åž‹è®­ç»ƒAPIè°ƒæ•´ï¼‰
    # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®žé™…éœ€è¦æ›´å®Œæ•´çš„è®­ç»ƒå¾ªçŽ¯
    model.fit(x_train_selected, y_train_selected, 
              validation_data=(x_valid, y_valid),
              epochs=3,  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«é€Ÿåº¦
              batch_size=32)
    
    # è¯„ä¼°æ€§èƒ½
    train_score = model.score(x_train_selected, y_train_selected)
    valid_score = model.score(x_valid, y_valid)
    
    return {
        "train_score": train_score,
        "valid_score": valid_score,
        "selected_samples": len(selected_indices),
        "original_samples": len(x_train)
    }


def create_comparison_plots(results, output_dir="./results"):
    """Create comparison plots"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Filter successful results
    successful_results = [r for r in results if r["status"] == "success"]
    
    if not successful_results:
        print("âš ï¸  No successful results for plotting")
        return
    
    # 1. Runtime comparison plot
    plt.figure(figsize=(12, 6))
    evaluator_names = [r["evaluator"] for r in successful_results]
    durations = [r["duration_seconds"] for r in successful_results]
    
    plt.barh(evaluator_names, durations)
    plt.xlabel("Runtime (seconds)")
    plt.title("Runtime Comparison of Data Evaluation Methods")
    plt.tight_layout()
    plt.savefig(output_path / "runtime_comparison.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # 2. Data values distribution plot
    plt.figure(figsize=(15, 10))
    n_methods = len(successful_results)
    cols = 3
    rows = (n_methods + cols - 1) // cols
    
    for i, result in enumerate(successful_results):
        plt.subplot(rows, cols, i + 1)
        data_values = np.array(result["data_values"])
        plt.hist(data_values, bins=30, alpha=0.7)
        plt.title(f'{result["evaluator"]}\n'
                 f'Mean: {result["data_values_stats"]["mean"]:.3f}')
        plt.xlabel("Data Values")
        plt.ylabel("Frequency")
    
    plt.tight_layout()
    plt.savefig(output_path / "data_values_distribution.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # 3. Data values statistics comparison plot
    plt.figure(figsize=(12, 8))
    means = [r["data_values_stats"]["mean"] for r in successful_results]
    stds = [r["data_values_stats"]["std"] for r in successful_results]
    
    x_pos = np.arange(len(evaluator_names))
    plt.errorbar(x_pos, means, yerr=stds, fmt='o-', capsize=5)
    plt.xticks(x_pos, evaluator_names, rotation=45, ha='right')
    plt.ylabel("Data Values")
    plt.title("Data Values Statistics Comparison (Mean Â± Std)")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_path / "data_values_stats_comparison.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ðŸ“Š Comparison plots saved to: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ BERTæƒ…æ„Ÿåˆ†æžæ•°æ®è¯„ä¼°å®žéªŒ - æ‰€æœ‰æ–¹æ³•ï¼ˆé™¤TIMå¤–ï¼‰")
    print("=" * 80)
    
    # å®žéªŒé…ç½®
    config = {
        "dataset": "imdb",
        "train_count": 10,
        "test_count": 10,
        "noise_rate": 0.3,
        "random_state": 42,
        "metric": "accuracy",
        "output_dir": "./all_methods_results_fixed"
    }
    
    print(f"ðŸ“‹ å®žéªŒé…ç½®:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # å‡†å¤‡æ•°æ®
    print(f"\nðŸ”§ å‡†å¤‡å®žéªŒæ•°æ®...")
    data_fetcher = prepare_data(
        train_count=config["train_count"],
        test_count=config["test_count"],
        noise_rate=config["noise_rate"],
        random_state=config["random_state"]
    )
    
    # åˆ›å»ºæ¨¡åž‹å·¥åŽ‚å‡½æ•°
    def model_factory():
        return create_bert_model(num_classes=2)
    
    # èŽ·å–è¯„ä¼°å™¨é…ç½®
    evaluators_config = get_evaluator_configs()
    
    print(f"\nðŸ§ª å°†è¿è¡Œ {len(evaluators_config)} ä¸ªæ•°æ®è¯„ä¼°æ–¹æ³•:")
    for name, config_item in evaluators_config.items():
        print(f"   â€¢ {name}: {config_item['description']}")
    
    # è¿è¡Œå®žéªŒ
    results = []
    total_start_time = time.time()
    
    for evaluator_name, evaluator_config in evaluators_config.items():
        try:
            # ä¸ºæ¯ä¸ªè¯„ä¼°å™¨åˆ›å»ºæ–°çš„æ¨¡åž‹å®žä¾‹
            model = model_factory()
            
            # è¿è¡Œè¯„ä¼°å™¨
            result = run_evaluator(
                evaluator_name, 
                evaluator_config, 
                data_fetcher, 
                model, 
                metric=config["metric"]
            )
            
            results.append(result)
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å™¨ {evaluator_name} è¿è¡Œå¤±è´¥: {e}")
            results.append({
                "evaluator": evaluator_name,
                "status": "failed",
                "error": str(e)
            })
    
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    # ä¿å­˜ç»“æžœ
    output_dir = Path(config["output_dir"])
    output_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜å®Œæ•´ç»“æžœ
    with open(output_dir / "all_methods_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "config": config,
            "results": results,
            "total_duration_seconds": total_duration,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    create_comparison_plots(results, output_dir)
    
    # æ‰“å°æ€»ç»“
    print(f"\nðŸŽ‰ å®žéªŒå®Œæˆ!")
    print("=" * 80)
    
    successful = [r for r in results if r["status"] == "success"]
    failed = [r for r in results if r["status"] == "failed"]
    
    print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)")
    print(f"âœ… æˆåŠŸçš„æ–¹æ³•: {len(successful)}/{len(results)}")
    print(f"âŒ å¤±è´¥çš„æ–¹æ³•: {len(failed)}")
    
    if successful:
        print(f"\nðŸ“Š æˆåŠŸæ–¹æ³•ç»“æžœ:")
        for result in successful:
            stats = result["data_values_stats"]
            print(f"   â€¢ {result['evaluator']}: "
                  f"å‡å€¼={stats['mean']:.4f}, "
                  f"ç”¨æ—¶={result['duration_seconds']:.1f}s")
    
    if failed:
        print(f"\nâš ï¸  å¤±è´¥æ–¹æ³•:")
        for result in failed:
            print(f"   â€¢ {result['evaluator']}: {result.get('error', 'Unknown error')}")
    
    print(f"\nðŸ“ ç»“æžœä¿å­˜ä½ç½®: {output_dir}")
    print(f"   â€¢ all_methods_results.json - å®Œæ•´ç»“æžœæ•°æ®")
    print(f"   â€¢ runtime_comparison.png - è¿è¡Œæ—¶é—´å¯¹æ¯”")
    print(f"   â€¢ data_values_distribution.png - æ•°æ®å€¼åˆ†å¸ƒ")
    print(f"   â€¢ data_values_stats_comparison.png - æ•°æ®å€¼ç»Ÿè®¡å¯¹æ¯”")
    
    return len(successful) > 0


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
"""
BERTè®­ç»ƒå’Œè¯„ä¼°æ¨¡å—

æä¾›æ¨¡å—åŒ–çš„BERTè®­ç»ƒã€è¯„ä¼°å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½, æ”¯æŒç›¸åŒåˆå§‹åŒ–çš„å¯¹æ¯”å®éªŒã€‚
"""

import copy
import time
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch

from opendataval.model import BertClassifier


class BertTrainer:
    """BERTè®­ç»ƒå™¨"""

    def __init__(
        self,
        model_name: str = "distilbert-base-uncased",
        num_classes: int = 2,
        dropout_rate: float = 0.2,
        num_train_layers: int = 2,
        device: str = "auto",
        random_state: int = 42,
    ):
        """
        åˆå§‹åŒ–BERTè®­ç»ƒå™¨

        Parameters:
        -----------
        model_name : str
            é¢„è®­ç»ƒæ¨¡å‹åç§°
        num_classes : int
            åˆ†ç±»ç±»åˆ«æ•°
        dropout_rate : float
            Dropoutç‡
        num_train_layers : int
            å¾®è°ƒå±‚æ•°
        device : str
            è®¾å¤‡é€‰æ‹© ('auto', 'cpu', 'cuda', 'mps')
        random_state : int
            éšæœºç§å­
        """
        self.model_name = model_name
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.num_train_layers = num_train_layers
        self.random_state = random_state

        # è®¾ç½®è®¾å¤‡
        self.device = self._setup_device(device)

        # è®¾ç½®éšæœºç§å­
        self._set_random_seed(random_state)

        # è®­ç»ƒå†å²
        self.training_history = defaultdict(list)

    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                device_obj = torch.device("cuda")
                print("ğŸš€ ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
            elif torch.backends.mps.is_available():
                device_obj = torch.device("mps")
                print("ğŸ ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
                # MPSä¼˜åŒ–è®¾ç½®
                import os

                os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            else:
                device_obj = torch.device("cpu")
                print("ğŸ’» ä½¿ç”¨CPU")
        else:
            device_obj = torch.device(device)

        return device_obj

    def _set_random_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°"""
        torch.manual_seed(seed)
        np.random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        # ç¡®ä¿CuDNNçš„ç¡®å®šæ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    def create_model(self) -> BertClassifier:
        """åˆ›å»ºBERTæ¨¡å‹"""
        print(f"ğŸ¤– åˆ›å»ºBERTæ¨¡å‹: {self.model_name}")

        model = BertClassifier(
            pretrained_model_name=self.model_name,
            num_classes=self.num_classes,
            dropout_rate=self.dropout_rate,
            num_train_layers=self.num_train_layers,
        )

        model = model.to(self.device)
        print(f"ğŸ“ æ¨¡å‹è®¾å¤‡: {self.device}")

        return model

    def save_model_state(self, model: BertClassifier) -> Dict:
        """ä¿å­˜æ¨¡å‹çŠ¶æ€ (ç”¨äºç›¸åŒåˆå§‹åŒ–)"""
        return copy.deepcopy(model.state_dict())

    def load_model_state(self, model: BertClassifier, state_dict: Dict):
        """åŠ è½½æ¨¡å‹çŠ¶æ€ (ç”¨äºç›¸åŒåˆå§‹åŒ–)"""
        model.load_state_dict(state_dict)
        return model

    def train_model(
        self,
        model: BertClassifier,
        data: Dict,
        epochs: int = 5,
        batch_size: int = 16,
        learning_rate: float = 2e-5,
        warmup_steps: int = 100,
        max_grad_norm: float = 1.0,
        eval_steps: int = 50,
        save_steps: int = 100,
        log_steps: int = 10,
    ) -> Dict:
        """
        è®­ç»ƒBERTæ¨¡å‹

        Parameters:
        -----------
        model : BertClassifier
            è¦è®­ç»ƒçš„æ¨¡å‹
        data : Dict
            åŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸ {'x_train', 'y_train', 'x_valid', 'y_valid'}
        epochs : int
            è®­ç»ƒè½®æ•°
        batch_size : int
            æ‰¹æ¬¡å¤§å°
        learning_rate : float
            å­¦ä¹ ç‡
        warmup_steps : int
            é¢„çƒ­æ­¥æ•°
        max_grad_norm : float
            æ¢¯åº¦è£å‰ªé˜ˆå€¼
        eval_steps : int
            è¯„ä¼°é—´éš”
        save_steps : int
            ä¿å­˜é—´éš”
        log_steps : int
            æ—¥å¿—é—´éš”

        Returns:
        --------
        Dict: è®­ç»ƒå†å²å’Œæœ€ç»ˆæ€§èƒ½
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒBERTæ¨¡å‹")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(data['y_train'])}")
        print(f"   éªŒè¯æ ·æœ¬: {len(data['y_valid'])}")
        print(f"   è½®æ•°: {epochs}, æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")

        # å‡†å¤‡æ•°æ®
        x_train, y_train = data["x_train"], data["y_train"]
        x_valid, y_valid = data["x_valid"], data["y_valid"]

        # ç¡®ä¿æ ‡ç­¾åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        y_train = y_train.to(self.device)
        y_valid = y_valid.to(self.device)

        # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)

        # è®¡ç®—æ€»æ­¥æ•°
        steps_per_epoch = len(x_train) // batch_size + (
            1 if len(x_train) % batch_size > 0 else 0
        )
        total_steps = epochs * steps_per_epoch

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=1, end_factor=0.1, total_iters=total_steps
        )

        # è®­ç»ƒå†å²è®°å½•
        history = {
            "train_loss": [],
            "train_accuracy": [],
            "valid_loss": [],
            "valid_accuracy": [],
            "learning_rates": [],
            "timestamps": [],
            "step_losses": [],  # æ¯æ­¥çš„loss, ç”¨äºç»˜å›¾
            "step_accuracies": [],  # æ¯æ­¥çš„accuracy
        }

        model.train()
        start_time = time.time()
        step = 0

        for epoch in range(epochs):
            epoch_losses = []
            epoch_correct = 0
            epoch_total = 0

            print(f"\nğŸ“ Epoch {epoch + 1}/{epochs}")

            # æ‰¹æ¬¡è®­ç»ƒ
            for batch_start in range(0, len(x_train), batch_size):
                batch_end = min(batch_start + batch_size, len(x_train))
                batch_x = x_train[batch_start:batch_end]
                batch_y = y_train[batch_start:batch_end]

                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()

                try:
                    # BERTæ¨¡å‹éœ€è¦tokenization
                    outputs = model.predict(batch_x)  # ä½¿ç”¨predictæ–¹æ³•

                    # è®¡ç®—æŸå¤±
                    loss = torch.nn.functional.cross_entropy(outputs, batch_y)

                    # åå‘ä¼ æ’­
                    loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

                    # ä¼˜åŒ–æ­¥éª¤
                    optimizer.step()
                    scheduler.step()

                    # è®°å½•æŸå¤±
                    epoch_losses.append(loss.item())
                    history["step_losses"].append(loss.item())

                    # è®¡ç®—å‡†ç¡®ç‡
                    _, predicted = torch.max(outputs, 1)
                    correct = (predicted == batch_y).sum().item()
                    accuracy = correct / len(batch_y)

                    epoch_correct += correct
                    epoch_total += len(batch_y)
                    history["step_accuracies"].append(accuracy)

                    step += 1

                    # æ—¥å¿—è®°å½•
                    if step % log_steps == 0:
                        current_lr = scheduler.get_last_lr()[0]
                        print(
                            f"   Step {step}/{total_steps}: Loss={loss.item():.4f}, "
                            f"Acc={accuracy:.4f}, LR={current_lr:.2e}"
                        )

                    # éªŒè¯è¯„ä¼°
                    if step % eval_steps == 0:
                        valid_loss, valid_acc = self.evaluate_model(
                            model, x_valid, y_valid
                        )
                        history["valid_loss"].append(valid_loss)
                        history["valid_accuracy"].append(valid_acc)
                        print(
                            f"   ğŸ” éªŒè¯ - Loss: {valid_loss:.4f}, Accuracy: {valid_acc:.4f}"
                        )
                        model.train()  # å›åˆ°è®­ç»ƒæ¨¡å¼

                except Exception as e:
                    print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
                    continue

            # Epochç»“æŸç»Ÿè®¡
            epoch_loss = np.mean(epoch_losses)
            epoch_acc = epoch_correct / epoch_total

            history["train_loss"].append(epoch_loss)
            history["train_accuracy"].append(epoch_acc)
            history["learning_rates"].append(scheduler.get_last_lr()[0])
            history["timestamps"].append(time.time() - start_time)

            print(
                f"âœ… Epoch {epoch + 1} å®Œæˆ - "
                f"Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}"
            )

        # æœ€ç»ˆè¯„ä¼°
        final_train_loss, final_train_acc = self.evaluate_model(model, x_train, y_train)
        final_valid_loss, final_valid_acc = self.evaluate_model(model, x_valid, y_valid)

        total_time = time.time() - start_time

        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print("ğŸ“Š æœ€ç»ˆæ€§èƒ½:")
        print(
            f"   è®­ç»ƒé›† - Loss: {final_train_loss:.4f}, Accuracy: {final_train_acc:.4f}"
        )
        print(
            f"   éªŒè¯é›† - Loss: {final_valid_loss:.4f}, Accuracy: {final_valid_acc:.4f}"
        )

        # æ·»åŠ æœ€ç»ˆæ€§èƒ½åˆ°å†å²è®°å½•
        history["final_performance"] = {
            "train_loss": final_train_loss,
            "train_accuracy": final_train_acc,
            "valid_loss": final_valid_loss,
            "valid_accuracy": final_valid_acc,
            "total_time": total_time,
            "total_steps": step,
        }

        return history

    def evaluate_model(
        self,
        model: BertClassifier,
        x_data: List,
        y_data: torch.Tensor,
        batch_size: int = 32,
    ) -> Tuple[float, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Parameters:
        -----------
        model : BertClassifier
            è¦è¯„ä¼°çš„æ¨¡å‹
        x_data : List
            è¾“å…¥æ•°æ®
        y_data : torch.Tensor
            æ ‡ç­¾æ•°æ®
        batch_size : int
            è¯„ä¼°æ‰¹æ¬¡å¤§å°

        Returns:
        --------
        Tuple[float, float]: (å¹³å‡æŸå¤±, å‡†ç¡®ç‡)
        """
        model.eval()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        y_data = y_data.to(self.device)

        with torch.no_grad():
            for batch_start in range(0, len(x_data), batch_size):
                batch_end = min(batch_start + batch_size, len(x_data))
                batch_x = x_data[batch_start:batch_end]
                batch_y = y_data[batch_start:batch_end]

                try:
                    outputs = model.predict(batch_x)
                    loss = torch.nn.functional.cross_entropy(outputs, batch_y)

                    total_loss += loss.item() * len(batch_y)

                    _, predicted = torch.max(outputs, 1)
                    total_correct += (predicted == batch_y).sum().item()
                    total_samples += len(batch_y)

                except Exception as e:
                    print(f"âš ï¸  è¯„ä¼°æ‰¹æ¬¡å¤±è´¥: {e}")
                    continue

        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        accuracy = total_correct / total_samples if total_samples > 0 else 0.0

        return avg_loss, accuracy

    def save_training_history(self, history: Dict, save_path: str):
        """ä¿å­˜è®­ç»ƒå†å²"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºnumpyæ ¼å¼ä¾¿äºåˆ†æ
        np.savez(
            save_path / "training_history.npz",
            **{
                k: np.array(v) if isinstance(v, list) else v
                for k, v in history.items()
                if k != "final_performance"
            },
        )

        # ä¿å­˜æœ€ç»ˆæ€§èƒ½ä¸ºJSON
        import json

        if "final_performance" in history:
            with open(save_path / "final_performance.json", "w") as f:
                json.dump(history["final_performance"], f, indent=2)

        print(f"ğŸ’¾ è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {save_path}")


def create_bert_trainer(
    model_name: str = "distilbert-base-uncased",
    num_classes: int = 2,
    dropout_rate: float = 0.2,
    num_train_layers: int = 2,
    device: str = "auto",
    random_state: int = 42,
) -> BertTrainer:
    """
    å·¥å‚å‡½æ•°: åˆ›å»ºBERTè®­ç»ƒå™¨

    Parameters:
    -----------
    model_name : str
        é¢„è®­ç»ƒæ¨¡å‹åç§°, é»˜è®¤"distilbert-base-uncased"
    num_classes : int
        åˆ†ç±»ç±»åˆ«æ•°, é»˜è®¤2
    dropout_rate : float
        Dropoutç‡, é»˜è®¤0.2
    num_train_layers : int
        å¾®è°ƒå±‚æ•°, é»˜è®¤2
    device : str
        è®¾å¤‡é€‰æ‹©, é»˜è®¤"auto"
    random_state : int
        éšæœºç§å­, é»˜è®¤42

    Returns:
    --------
    BertTrainer
        é…ç½®å¥½çš„è®­ç»ƒå™¨
    """
    return BertTrainer(
        model_name=model_name,
        num_classes=num_classes,
        dropout_rate=dropout_rate,
        num_train_layers=num_train_layers,
        device=device,
        random_state=random_state,
    )


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒå™¨
    print("ğŸ§ª æµ‹è¯•BERTè®­ç»ƒå™¨")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = create_bert_trainer()

    # åˆ›å»ºæ¨¡å‹
    model = trainer.create_model()

    print("âœ… BERTè®­ç»ƒå™¨æµ‹è¯•å®Œæˆ")

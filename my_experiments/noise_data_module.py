"""
æ•°æ®å™ªå£°æ³¨å…¥å’Œå¤„ç†æ¨¡å—

ä½¿ç”¨OpenDataValçš„å™ªå£°æ³¨å…¥APIæ¥å¤„ç†æ ‡ç­¾å™ªå£°ï¼Œæ”¯æŒæ¨¡å—åŒ–çš„æ•°æ®å¤„ç†æµç¨‹ã€‚
"""

import numpy as np
import torch
from typing import Tuple, List, Optional
from pathlib import Path

from opendataval.dataloader import DataFetcher
from opendataval.dataloader.noisify import add_gauss_noise, add_noise


class NoiseDataProcessor:
    """æ•°æ®å™ªå£°æ³¨å…¥å’Œå¤„ç†å™¨"""
    
    def __init__(
        self, 
        dataset_name: str = "imdb",
        train_count: int = 1000,
        valid_count: int = 200,
        test_count: int = 200,
        noise_rate: float = 0.3,
        random_state: int = 42
    ):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Parameters:
        -----------
        dataset_name : str
            æ•°æ®é›†åç§°
        train_count : int
            è®­ç»ƒæ ·æœ¬æ•°é‡
        valid_count : int 
            éªŒè¯æ ·æœ¬æ•°é‡
        test_count : int
            æµ‹è¯•æ ·æœ¬æ•°é‡
        noise_rate : float
            æ ‡ç­¾å™ªå£°æ¯”ä¾‹ (0.0-1.0)
        random_state : int
            éšæœºç§å­
        """
        self.dataset_name = dataset_name
        self.train_count = train_count
        self.valid_count = valid_count
        self.test_count = test_count
        self.noise_rate = noise_rate
        self.random_state = random_state
        
        # æ•°æ®å­˜å‚¨
        self.clean_data = None
        self.noisy_data = None
        self.noise_indices = None
        
    def load_clean_data(self) -> Tuple:
        """åŠ è½½å¹²å‡€çš„æ•°æ®"""
        print(f"ğŸ”„ åŠ è½½å¹²å‡€æ•°æ®: {self.dataset_name}")
        print(f"ğŸ“Š æ•°æ®è§„æ¨¡: è®­ç»ƒ={self.train_count}, éªŒè¯={self.valid_count}, æµ‹è¯•={self.test_count}")
        
        # ä½¿ç”¨DataFetcheråŠ è½½æ•°æ®
        fetcher = DataFetcher.setup(
            dataset_name=self.dataset_name,
            train_count=self.train_count,
            valid_count=self.valid_count,
            test_count=self.test_count,
            random_state=self.random_state
        )
        
        x_train, y_train, x_valid, y_valid, x_test, y_test = fetcher.datapoints
        
        # æ•°æ®æ ¼å¼å¤„ç†
        x_train_data, y_train_clean = self._process_data_format(x_train, y_train)
        x_valid_data, y_valid_clean = self._process_data_format(x_valid, y_valid)
        x_test_data, y_test_clean = self._process_data_format(x_test, y_test)
        
        self.clean_data = {
            'x_train': x_train_data,
            'y_train': y_train_clean,
            'x_valid': x_valid_data,
            'y_valid': y_valid_clean,
            'x_test': x_test_data,
            'y_test': y_test_clean
        }
        
        print("âœ… å¹²å‡€æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(x_train_data)}")
        print(f"   éªŒè¯é›†æ ·æœ¬æ•°: {len(x_valid_data)}")
        print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(x_test_data)}")
        
        return self.clean_data
    
    def inject_label_noise(self, noise_type: str = "uniform") -> Tuple:
        """
        æ³¨å…¥æ ‡ç­¾å™ªå£°
        
        Parameters:
        -----------
        noise_type : str
            å™ªå£°ç±»å‹: 'uniform' æˆ– 'class_dependent'
            
        Returns:
        --------
        Tuple: (noisy_data, noise_indices)
        """
        if self.clean_data is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_clean_data()åŠ è½½æ•°æ®")
            
        print(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£°: {self.noise_rate*100:.1f}% ({noise_type})")
        
        y_train_clean = self.clean_data['y_train']
        n_samples = len(y_train_clean)
        n_noisy = int(n_samples * self.noise_rate)
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.random_state)
        torch.manual_seed(self.random_state)
        
        # é€‰æ‹©è¦åŠ å™ªå£°çš„æ ·æœ¬
        noise_indices = np.random.choice(n_samples, size=n_noisy, replace=False)
        
        # åˆ›å»ºå™ªå£°æ ‡ç­¾
        y_train_noisy = y_train_clean.clone()
        
        if noise_type == "uniform":
            # å‡åŒ€æ ‡ç­¾ç¿»è½¬
            for idx in noise_indices:
                # å¯¹äºäºŒåˆ†ç±»ï¼Œç›´æ¥ç¿»è½¬æ ‡ç­¾
                y_train_noisy[idx] = 1 - y_train_noisy[idx]
                
        elif noise_type == "class_dependent":
            # ç±»åˆ«ç›¸å…³å™ªå£°ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
            for idx in noise_indices:
                # ç®€åŒ–ç‰ˆï¼šä¹Ÿæ˜¯ç¿»è½¬ï¼Œä½†å¯ä»¥æ ¹æ®ç±»åˆ«è®¾ç½®ä¸åŒå™ªå£°ç‡
                y_train_noisy[idx] = 1 - y_train_noisy[idx]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å™ªå£°ç±»å‹: {noise_type}")
        
        # åˆ›å»ºå™ªå£°æ•°æ®å‰¯æœ¬
        self.noisy_data = self.clean_data.copy()
        self.noisy_data['y_train'] = y_train_noisy
        self.noise_indices = noise_indices
        
        # ç»Ÿè®¡å™ªå£°ä¿¡æ¯
        actually_flipped = torch.sum(y_train_clean != y_train_noisy).item()
        
        print(f"âœ… å™ªå£°æ³¨å…¥å®Œæˆ")
        print(f"   ç›®æ ‡å™ªå£°æ ·æœ¬: {n_noisy}")
        print(f"   å®é™…ç¿»è½¬æ ·æœ¬: {actually_flipped}")
        print(f"   å®é™…å™ªå£°ç‡: {actually_flipped/n_samples*100:.1f}%")
        
        return self.noisy_data, self.noise_indices
    
    def _process_data_format(self, x_data, y_data):
        """å¤„ç†æ•°æ®æ ¼å¼ï¼Œç»Ÿä¸€ä¸ºåˆ—è¡¨å’Œtensor"""
        # å¤„ç†ç‰¹å¾æ•°æ®
        if hasattr(x_data, 'dataset'):
            # å¦‚æœæ˜¯Subsetå¯¹è±¡ï¼Œæå–å®é™…æ•°æ®
            x_processed = [x_data.dataset[i] for i in x_data.indices]
        else:
            x_processed = list(x_data)
            
        # å¤„ç†æ ‡ç­¾æ•°æ®
        if not isinstance(y_data, torch.Tensor):
            y_processed = torch.tensor(y_data, dtype=torch.long)
        else:
            y_processed = y_data.clone()
            
        # å¦‚æœæ˜¯one-hotç¼–ç ï¼Œè½¬æ¢ä¸ºç´¢å¼•
        if len(y_processed.shape) > 1 and y_processed.shape[1] > 1:
            y_processed = torch.argmax(y_processed, dim=1)
            
        return x_processed, y_processed
    
    def get_noise_statistics(self) -> dict:
        """è·å–å™ªå£°ç»Ÿè®¡ä¿¡æ¯"""
        if self.noisy_data is None or self.noise_indices is None:
            return {"error": "å™ªå£°æ•°æ®æœªç”Ÿæˆ"}
            
        clean_labels = self.clean_data['y_train']
        noisy_labels = self.noisy_data['y_train']
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        stats = {
            'total_samples': len(clean_labels),
            'noise_indices': self.noise_indices.tolist(),
            'noise_count': len(self.noise_indices),
            'noise_rate': len(self.noise_indices) / len(clean_labels),
            'actually_flipped': torch.sum(clean_labels != noisy_labels).item(),
            'class_distribution': {
                'clean': {
                    'positive': torch.sum(clean_labels == 1).item(),
                    'negative': torch.sum(clean_labels == 0).item()
                },
                'noisy': {
                    'positive': torch.sum(noisy_labels == 1).item(),
                    'negative': torch.sum(noisy_labels == 0).item()
                }
            }
        }
        
        return stats
    
    def prune_data_by_indices(self, prune_indices: np.ndarray) -> Tuple:
        """
        æ ¹æ®ç´¢å¼•å‰ªææ•°æ®
        
        Parameters:
        -----------
        prune_indices : np.ndarray
            è¦ç§»é™¤çš„æ ·æœ¬ç´¢å¼•
            
        Returns:
        --------
        Tuple: (pruned_data, remaining_indices)
        """
        if self.noisy_data is None:
            raise ValueError("å™ªå£°æ•°æ®æœªç”Ÿæˆï¼Œè¯·å…ˆè°ƒç”¨inject_label_noise()")
            
        print(f"âœ‚ï¸  å‰ªææ•°æ®: ç§»é™¤ {len(prune_indices)} ä¸ªæ ·æœ¬")
        
        # è·å–ä¿ç•™çš„ç´¢å¼•
        all_indices = np.arange(len(self.noisy_data['y_train']))
        remaining_indices = np.setdiff1d(all_indices, prune_indices)
        
        # åˆ›å»ºå‰ªæåçš„æ•°æ®
        pruned_data = {}
        
        # å¤„ç†è®­ç»ƒæ•°æ®
        x_train_pruned = [self.noisy_data['x_train'][i] for i in remaining_indices]
        y_train_pruned = self.noisy_data['y_train'][remaining_indices]
        
        pruned_data = {
            'x_train': x_train_pruned,
            'y_train': y_train_pruned,
            'x_valid': self.noisy_data['x_valid'],  # éªŒè¯é›†ä¸å˜
            'y_valid': self.noisy_data['y_valid'],
            'x_test': self.noisy_data['x_test'],    # æµ‹è¯•é›†ä¸å˜  
            'y_test': self.noisy_data['y_test']
        }
        
        print(f"âœ… å‰ªæå®Œæˆ")
        print(f"   åŸå§‹è®­ç»ƒæ ·æœ¬: {len(self.noisy_data['y_train'])}")
        print(f"   å‰ªæåæ ·æœ¬: {len(y_train_pruned)}")
        print(f"   ä¿ç•™ç‡: {len(remaining_indices)/len(all_indices)*100:.1f}%")
        
        return pruned_data, remaining_indices
    
    def save_data(self, data: dict, save_path: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºpytorchæ ¼å¼
        torch.save(data, save_path / "processed_data.pt")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self, 'noise_indices') and self.noise_indices is not None:
            stats = self.get_noise_statistics()
            import json
            with open(save_path / "noise_stats.json", 'w') as f:
                json.dump(stats, f, indent=2)
                
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {save_path}")


def create_noise_processor(
    dataset_name: str = "imdb",
    train_count: int = 1000,
    valid_count: int = 200, 
    test_count: int = 200,
    noise_rate: float = 0.3,
    random_state: int = 42
) -> NoiseDataProcessor:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå™ªå£°æ•°æ®å¤„ç†å™¨
    
    Parameters:
    -----------
    dataset_name : str
        æ•°æ®é›†åç§°ï¼Œé»˜è®¤"imdb"
    train_count : int
        è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œé»˜è®¤1000
    valid_count : int
        éªŒè¯æ ·æœ¬æ•°é‡ï¼Œé»˜è®¤200
    test_count : int
        æµ‹è¯•æ ·æœ¬æ•°é‡ï¼Œé»˜è®¤200
    noise_rate : float
        æ ‡ç­¾å™ªå£°æ¯”ä¾‹ï¼Œé»˜è®¤0.3 (30%)
    random_state : int
        éšæœºç§å­ï¼Œé»˜è®¤42
        
    Returns:
    --------
    NoiseDataProcessor
        é…ç½®å¥½çš„æ•°æ®å¤„ç†å™¨
    """
    return NoiseDataProcessor(
        dataset_name=dataset_name,
        train_count=train_count,
        valid_count=valid_count,
        test_count=test_count,
        noise_rate=noise_rate,
        random_state=random_state
    )


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®å¤„ç†å™¨
    processor = create_noise_processor(
        train_count=100,
        valid_count=50,
        test_count=50,
        noise_rate=0.3
    )
    
    # åŠ è½½æ•°æ®
    clean_data = processor.load_clean_data()
    
    # æ³¨å…¥å™ªå£°
    noisy_data, noise_indices = processor.inject_label_noise()
    
    # è·å–ç»Ÿè®¡
    stats = processor.get_noise_statistics()
    print(f"ğŸ“Š å™ªå£°ç»Ÿè®¡: {stats}")
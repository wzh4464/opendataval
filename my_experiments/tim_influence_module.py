"""
TIMå½±å“åŠ›è®¡ç®—æ¨¡å—

åŸºäºä¹‹å‰çš„TIMå®ç°, æä¾›æ¨¡å—åŒ–çš„å½±å“åŠ›è®¡ç®—åŠŸèƒ½, æ”¯æŒBERTæ¨¡å‹çš„æ•°æ®ä»·å€¼è¯„ä¼°ã€‚
"""

from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import torch

from opendataval.dataval.tim import TimInfluence
from opendataval.model import BertClassifier


class BertTimInfluenceCalculator:
    """BERT + TIM å½±å“åŠ›è®¡ç®—å™¨"""

    def __init__(
        self,
        t1: int = 0,
        t2: Optional[int] = None,
        num_epochs: int = 3,
        batch_size: int = 8,
        regularization: float = 0.01,
        finite_diff_eps: float = 1e-5,
        random_state: int = 42,
    ):
        """
        åˆå§‹åŒ–TIMå½±å“åŠ›è®¡ç®—å™¨

        Parameters:
        -----------
        t1 : int
            Time window start STEP (not epoch), default 0
        t2 : Optional[int]
            Time window end STEP (not epoch), None means to end (T)
        num_epochs : int
            è®­ç»ƒè½®æ•°, é»˜è®¤3
        batch_size : int
            æ‰¹æ¬¡å¤§å°, é»˜è®¤8 (BERTéœ€è¦è¾ƒå°batch size)
        regularization : float
            L2æ­£åˆ™åŒ–å‚æ•°, é»˜è®¤0.01
        finite_diff_eps : float
            æœ‰é™å·®åˆ†å‚æ•°, é»˜è®¤1e-5
        random_state : int
            éšæœºç§å­, é»˜è®¤42
        """
        self.t1 = t1
        self.t2 = t2
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.regularization = regularization
        self.finite_diff_eps = finite_diff_eps
        self.random_state = random_state

        # TIMè¯„ä¼°å™¨
        self.tim_evaluator = None
        self.influence_scores = None

    def setup_tim_evaluator(self) -> TimInfluence:
        """è®¾ç½®TIMè¯„ä¼°å™¨"""
        print("âš™ï¸  Setting up TIM evaluator")
        print(
            f"   Time window (steps): t1={self.t1}, t2={'T(end)' if self.t2 is None else self.t2}"
        )
        print(
            f"   Training config: epochs={self.num_epochs}, batch_size={self.batch_size}"
        )

        self.tim_evaluator = TimInfluence(
            start_step=self.t1,
            end_step=self.t2,
            time_window_type=(
                "full" if self.t1 == 0 and self.t2 is None else "custom_range"
            ),
            num_epochs=self.num_epochs,
            batch_size=self.batch_size,
            regularization=self.regularization,
            finite_diff_eps=self.finite_diff_eps,
            random_state=self.random_state,
        )

        return self.tim_evaluator

    def create_bert_wrapper(self, model: BertClassifier, attention_mask: torch.Tensor):
        """åˆ›å»ºTIMå…¼å®¹çš„BERTåŒ…è£…å™¨"""

        class BertTimWrapper(torch.nn.Module):
            """åŒ…è£…BERTæ¨¡å‹ä»¥å…¼å®¹TIMçš„tensorè¾“å…¥æ ¼å¼"""

            def __init__(self, bert_model, attention_mask):
                super().__init__()
                self.bert_model = bert_model
                self.attention_mask = attention_mask.detach()

            def forward(self, input_ids):
                # TIMä¼ é€’çš„æ˜¯float tensor, è½¬æ¢ä¸ºtoken IDs
                batch_size = input_ids.shape[0]
                device = input_ids.device

                # ä½¿ç”¨å¯¹åº”çš„attention maskç‰‡æ®µ
                mask = self.attention_mask[:batch_size].to(device)

                # å°†float tensorè½¬ä¸ºlong token IDs
                input_ids_long = input_ids.long()

                # è°ƒç”¨BERTå¹¶è·å–logits (ä¸è¦softmax)
                if hasattr(self.bert_model, "classifier"):
                    # è·å–åˆ†ç±»å™¨ä¹‹å‰çš„hidden states
                    hidden_states = self.bert_model.bert(
                        input_ids_long, attention_mask=mask
                    )[0]
                    pooled_output = hidden_states[:, 0]  # [CLS] token

                    # åªé€šè¿‡linearå±‚, ä¸è¦softmax
                    pre_linear = self.bert_model.classifier.pre_linear(pooled_output)
                    activated = self.bert_model.classifier.acti(pre_linear)
                    dropped = self.bert_model.classifier.dropout(activated)
                    logits = self.bert_model.classifier.linear(dropped)

                    return logits  # è¿”å›raw logitsè€Œä¸æ˜¯softmaxè¾“å‡º
                else:
                    return self.bert_model(input_ids_long, attention_mask=mask)

            def predict(self, input_ids):
                """TIMè°ƒç”¨çš„é¢„æµ‹æ¥å£"""
                with torch.enable_grad():
                    return self.forward(input_ids)

            def parameters(self):
                return self.bert_model.parameters()

            def named_parameters(self):
                return self.bert_model.named_parameters()

            def zero_grad(self):
                return self.bert_model.zero_grad()

            def train(self):
                self.bert_model.train()
                return self

            def eval(self):
                self.bert_model.eval()
                return self

        return BertTimWrapper(model, attention_mask)

    def compute_influence(self, model: BertClassifier, data: Dict) -> np.ndarray:
        """
        è®¡ç®—è®­ç»ƒæ•°æ®çš„å½±å“åŠ›åˆ†æ•°

        Parameters:
        -----------
        model : BertClassifier
            è®­ç»ƒå¥½çš„BERTæ¨¡å‹
        data : Dict
            åŒ…å«è®­ç»ƒå’ŒéªŒè¯æ•°æ®çš„å­—å…¸

        Returns:
        --------
        np.ndarray: å½±å“åŠ›åˆ†æ•°æ•°ç»„
        """
        print("ğŸ”„ è®¡ç®—TIMå½±å“åŠ›åˆ†æ•°")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(data['y_train'])}")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {len(data['y_valid'])}")

        # 1. è®¾ç½®TIMè¯„ä¼°å™¨
        if self.tim_evaluator is None:
            self.setup_tim_evaluator()

        # 2. å‡†å¤‡æ•°æ®
        x_train, y_train = data["x_train"], data["y_train"]
        x_valid, y_valid = data["x_valid"], data["y_valid"]

        # è·å–æ¨¡å‹è®¾å¤‡
        device = next(model.parameters()).device
        y_train = y_train.to(device)
        y_valid = y_valid.to(device)

        # 3. Tokenization
        print("   ğŸ”„ å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œtokenization...")
        train_dataset = model.tokenize(x_train)
        valid_dataset = model.tokenize(x_valid)

        # è·å–tokenizedæ•°æ®
        train_input_ids = train_dataset.tensors[0].to(device)
        train_attention_mask = train_dataset.tensors[1].to(device)
        valid_input_ids = valid_dataset.tensors[0].to(device)
        _ = valid_dataset.tensors[1].to(device)

        # 4. ä¸ºTIMå‡†å¤‡tensorè¾“å…¥ (è½¬æ¢ä¸ºfloatä»¥æ”¯æŒæ¢¯åº¦è®¡ç®—)
        self.tim_evaluator.input_data(
            x_train=train_input_ids.float(),
            y_train=y_train,
            x_valid=valid_input_ids.float(),
            y_valid=y_valid,
        )

        # 5. åˆ›å»ºBERTåŒ…è£…å™¨
        bert_wrapper = self.create_bert_wrapper(model, train_attention_mask)
        self.tim_evaluator.pred_model = bert_wrapper

        # 6. è®­ç»ƒå¹¶è®°å½•çŠ¶æ€
        print("   ğŸš€ å¼€å§‹TIMè®­ç»ƒ...")
        self.tim_evaluator.train_data_values(
            epochs=self.num_epochs,
            batch_size=self.batch_size,
            lr=2e-5,  # BERTæ¨èå­¦ä¹ ç‡
        )

        # 7. è®¡ç®—å½±å“åŠ›æ•°æ®å€¼
        print("   ğŸ“Š è®¡ç®—æ•°æ®å½±å“åŠ›...")
        influence_scores = self.tim_evaluator.evaluate_data_values()

        self.influence_scores = influence_scores

        print("âœ… å½±å“åŠ›è®¡ç®—å®Œæˆ")
        print(f"   å½±å“åŠ›åˆ†æ•°å½¢çŠ¶: {influence_scores.shape}")
        print(f"   å¹³å‡å½±å“åŠ›: {np.mean(influence_scores):.6f}")
        print(f"   æ ‡å‡†å·®: {np.std(influence_scores):.6f}")

        return influence_scores

    def analyze_influence_scores(
        self,
        influence_scores: np.ndarray,
        y_train: torch.Tensor,
        noise_indices: Optional[np.ndarray] = None,
    ) -> Dict:
        """
        åˆ†æå½±å“åŠ›åˆ†æ•°

        Parameters:
        -----------
        influence_scores : np.ndarray
            å½±å“åŠ›åˆ†æ•°
        y_train : torch.Tensor
            è®­ç»ƒæ ‡ç­¾
        noise_indices : Optional[np.ndarray]
            å™ªå£°æ ·æœ¬ç´¢å¼•

        Returns:
        --------
        Dict: åˆ†æç»“æœ
        """
        print("ğŸ“Š åˆ†æå½±å“åŠ›åˆ†æ•°...")

        # åŸºç¡€ç»Ÿè®¡
        stats = {
            "mean_influence": float(np.mean(influence_scores)),
            "std_influence": float(np.std(influence_scores)),
            "min_influence": float(np.min(influence_scores)),
            "max_influence": float(np.max(influence_scores)),
            "total_samples": len(influence_scores),
        }

        # æŒ‰ç±»åˆ«åˆ†æå½±å“åŠ›
        if isinstance(y_train, torch.Tensor):
            y_train_np = y_train.cpu().numpy()
        else:
            y_train_np = y_train

        positive_indices = np.where(y_train_np == 1)[0]
        negative_indices = np.where(y_train_np == 0)[0]

        stats["class_analysis"] = {
            "positive_samples": {
                "count": len(positive_indices),
                "mean_influence": float(np.mean(influence_scores[positive_indices])),
                "std_influence": float(np.std(influence_scores[positive_indices])),
            },
            "negative_samples": {
                "count": len(negative_indices),
                "mean_influence": float(np.mean(influence_scores[negative_indices])),
                "std_influence": float(np.std(influence_scores[negative_indices])),
            },
        }

        # æ‰¾å‡ºæœ€æœ‰å½±å“åŠ›å’Œæœ€æ— å½±å“åŠ›çš„æ ·æœ¬
        top_k = min(20, len(influence_scores))
        most_influential_indices = np.argsort(influence_scores)[-top_k:][::-1]
        least_influential_indices = np.argsort(influence_scores)[:top_k]

        stats["ranking"] = {
            "most_influential": {
                "indices": most_influential_indices.tolist(),
                "values": influence_scores[most_influential_indices].tolist(),
            },
            "least_influential": {
                "indices": least_influential_indices.tolist(),
                "values": influence_scores[least_influential_indices].tolist(),
            },
        }

        # å¦‚æœæœ‰å™ªå£°ä¿¡æ¯, åˆ†æå™ªå£°vså¹²å‡€æ ·æœ¬çš„å½±å“åŠ›
        if noise_indices is not None:
            clean_indices = np.setdiff1d(
                np.arange(len(influence_scores)), noise_indices
            )

            stats["noise_analysis"] = {
                "noise_samples": {
                    "count": len(noise_indices),
                    "mean_influence": float(np.mean(influence_scores[noise_indices])),
                    "std_influence": float(np.std(influence_scores[noise_indices])),
                },
                "clean_samples": {
                    "count": len(clean_indices),
                    "mean_influence": float(np.mean(influence_scores[clean_indices])),
                    "std_influence": float(np.std(influence_scores[clean_indices])),
                },
            }

            # å™ªå£°æ ·æœ¬åœ¨å½±å“åŠ›æ’åä¸­çš„ä½ç½®
            sorted_indices = np.argsort(influence_scores)
            noise_ranks = []
            for noise_idx in noise_indices:
                rank = np.where(sorted_indices == noise_idx)[0][0]
                rank_percentile = rank / len(influence_scores)
                noise_ranks.append(rank_percentile)

            stats["noise_analysis"]["noise_rank_percentiles"] = noise_ranks
            stats["noise_analysis"]["mean_noise_rank_percentile"] = float(
                np.mean(noise_ranks)
            )

        print("âœ… å½±å“åŠ›åˆ†æå®Œæˆ")
        return stats

    def select_bottom_k_samples(
        self, influence_scores: np.ndarray, k_ratio: float = 0.3
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        é€‰æ‹©å½±å“åŠ›æœ€ä½çš„k%æ ·æœ¬è¿›è¡Œå‰ªæ

        Parameters:
        -----------
        influence_scores : np.ndarray
            å½±å“åŠ›åˆ†æ•°
        k_ratio : float
            è¦å‰ªæçš„æ ·æœ¬æ¯”ä¾‹ (0.0-1.0)

        Returns:
        --------
        Tuple[np.ndarray, np.ndarray]: (å‰ªæç´¢å¼•, ä¿ç•™ç´¢å¼•)
        """
        n_samples = len(influence_scores)
        k = int(n_samples * k_ratio)

        # æŒ‰å½±å“åŠ›æ’åº, é€‰æ‹©æœ€ä½çš„kä¸ª
        sorted_indices = np.argsort(influence_scores)
        prune_indices = sorted_indices[:k]
        keep_indices = sorted_indices[k:]

        print("âœ‚ï¸  é€‰æ‹©å‰ªææ ·æœ¬")
        print(f"   æ€»æ ·æœ¬æ•°: {n_samples}")
        print(f"   å‰ªææ¯”ä¾‹: {k_ratio*100:.1f}%")
        print(f"   å‰ªææ ·æœ¬æ•°: {k}")
        print(f"   ä¿ç•™æ ·æœ¬æ•°: {len(keep_indices)}")
        print(
            f"   å‰ªææ ·æœ¬å½±å“åŠ›èŒƒå›´: [{influence_scores[prune_indices].min():.6f}, "
            f"{influence_scores[prune_indices].max():.6f}]"
        )

        return prune_indices, keep_indices

    def save_influence_results(
        self, influence_scores: np.ndarray, analysis: Dict, save_path: str
    ):
        """ä¿å­˜å½±å“åŠ›è®¡ç®—ç»“æœ"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜å½±å“åŠ›åˆ†æ•°
        np.save(save_path / "influence_scores.npy", influence_scores)

        # ä¿å­˜åˆ†æç»“æœ
        import json

        with open(save_path / "influence_analysis.json", "w") as f:
            json.dump(analysis, f, indent=2)

        print(f"ğŸ’¾ å½±å“åŠ›ç»“æœå·²ä¿å­˜åˆ°: {save_path}")


def create_tim_calculator(
    t1: int = 0,
    t2: Optional[int] = None,
    num_epochs: int = 3,
    batch_size: int = 8,
    regularization: float = 0.01,
    finite_diff_eps: float = 1e-5,
    random_state: int = 42,
) -> BertTimInfluenceCalculator:
    """
    å·¥å‚å‡½æ•°: åˆ›å»ºTIMå½±å“åŠ›è®¡ç®—å™¨

    Parameters:
    -----------
    t1 : int
        æ—¶é—´çª—å£å¼€å§‹æ­¥éª¤, é»˜è®¤0
    t2 : Optional[int]
        æ—¶é—´çª—å£ç»“æŸæ­¥éª¤, é»˜è®¤None (åˆ°ç»“æŸ)
    num_epochs : int
        è®­ç»ƒè½®æ•°, é»˜è®¤3
    batch_size : int
        æ‰¹æ¬¡å¤§å°, é»˜è®¤8
    regularization : float
        L2æ­£åˆ™åŒ–å‚æ•°, é»˜è®¤0.01
    finite_diff_eps : float
        æœ‰é™å·®åˆ†å‚æ•°, é»˜è®¤1e-5
    random_state : int
        éšæœºç§å­, é»˜è®¤42

    Returns:
    --------
    BertTimInfluenceCalculator
        é…ç½®å¥½çš„å½±å“åŠ›è®¡ç®—å™¨
    """
    return BertTimInfluenceCalculator(
        t1=t1,
        t2=t2,
        num_epochs=num_epochs,
        batch_size=batch_size,
        regularization=regularization,
        finite_diff_eps=finite_diff_eps,
        random_state=random_state,
    )


if __name__ == "__main__":
    # æµ‹è¯•TIMè®¡ç®—å™¨
    print("ğŸ§ª æµ‹è¯•TIMå½±å“åŠ›è®¡ç®—å™¨")

    calculator = create_tim_calculator()
    print("âœ… TIMè®¡ç®—å™¨åˆ›å»ºæˆåŠŸ")

#!/usr/bin/env python3
"""
LAVA + BERT æƒ…æ„Ÿåˆ†æ CLI

åŠŸèƒ½
----
- è¿è¡Œ LAVA æ•°æ®ä»·å€¼è¯„ä¼°, ä½¿ç”¨ BERT ä½œä¸ºåµŒå…¥æ¨¡å‹ (é»˜è®¤è¾“å‡ºCLS pooled embedding) ã€‚
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è°ƒæ•´æ•°æ®è§„æ¨¡ã€epochã€å­¦ä¹ ç‡ã€batch sizeã€è®¾å¤‡ç­‰ã€‚
- æ”¯æŒæŒ‰é˜¶æ®µ [0, t] (t <= T) è®¡ç®—å½±å“åŠ›:
  å¯¹æ¯ä¸ª t, ä½¿ç”¨ t ä½œä¸º fine-tune çš„æ€» epoch æ•°æ¥è®­ç»ƒ/å¾®è°ƒ BERT, ç„¶åè®¡ç®—ä¸€æ¬¡ LAVAã€‚

æ³¨æ„
----
- LAVA è‡ªèº«ä¸ä¾èµ–æ—¶é—´çª—å£; è¿™é‡Œçš„é˜¶æ®µæ˜¯é€šè¿‡â€œå°† fine-tune è½®æ•°è®¾ä¸º tâ€æ¥å¾—åˆ°ä¸åŒé˜¶æ®µçš„åµŒå…¥, å†è®¡ç®— LAVAã€‚
- è‹¥ t=0, åˆ™ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒ BERT (ä¸å¾®è°ƒ) ã€‚
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import List

import numpy as np
import torch

from opendataval.dataloader import DataFetcher
from opendataval.dataval.lava import LavaEvaluator
from opendataval.model import BertClassifier


def select_device(pref: str = "auto", gpu: Optional[int] = None) -> torch.device:
    if pref == "cuda" and torch.cuda.is_available():
        if gpu is not None and 0 <= gpu < torch.cuda.device_count():
            return torch.device(f"cuda:{gpu}")
        return torch.device("cuda")
    if pref == "mps" and torch.backends.mps.is_available():
        return torch.device("mps")
    if pref == "cpu":
        return torch.device("cpu")
    # auto
    if torch.cuda.is_available():
        if gpu is not None and 0 <= gpu < torch.cuda.device_count():
            return torch.device(f"cuda:{gpu}")
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def set_seed(seed: int):
    torch.manual_seed(seed)
    np.random.seed(seed)


class BertEmbeddingWrapper(torch.nn.Module):
    """åŒ…è£… BertClassifier, ä½¿å…¶ predict è¿”å›å¯ç”¨ä½œ LAVA ç‰¹å¾çš„å‘é‡ã€‚

    æ¨¡å¼:
    - pooled: è¿”å› CLS pooled embedding (é¦– token å¯¹åº”çš„éšè—çŠ¶æ€) ã€‚
    - logits: è¿”å›åˆ†ç±»å™¨çº¿æ€§å±‚è¾“å‡º (æœª softmax çš„ logits) ã€‚
    - probs: è¿”å›åˆ†ç±»æ¦‚ç‡ (BertClassifier.predict çš„é»˜è®¤è¾“å‡º) ã€‚
    """

    def __init__(self, bert_model: BertClassifier, mode: str = "pooled"):
        super().__init__()
        assert mode in {"pooled", "logits", "probs"}
        self.bert_model = bert_model
        self.mode = mode

    @property
    def device(self) -> torch.device:
        return self.bert_model.bert.device

    def predict(self, x_dataset) -> torch.Tensor:
        # x_dataset æ˜¯ ListDataset[str] æˆ–å…¼å®¹çš„ Dataset
        # ç»Ÿä¸€èµ° tokenization, ç„¶åæŒ‰éœ€è¦è¿”å›ç‰¹å¾
        if len(x_dataset) == 0:
            return torch.zeros(0, 1, device=self.device)

        # å°†æ–‡æœ¬è½¬ä¸º input_ids & attention_mask
        bert_inputs = self.bert_model.tokenize(x_dataset)
        input_ids, attention_mask = [t.to(self.device) for t in bert_inputs.tensors]

        with torch.no_grad():
            if self.mode == "pooled":
                hidden_states = self.bert_model.bert(
                    input_ids=input_ids, attention_mask=attention_mask
                )[0]
                pooled = hidden_states[:, 0]
                return pooled
            elif self.mode == "logits":
                # å¤ç”¨åˆ†ç±»å™¨, ä½†æˆªæ–­ softmax: åªå–çº¿æ€§å±‚ logits
                hidden_states = self.bert_model.bert(
                    input_ids=input_ids, attention_mask=attention_mask
                )[0]
                pooled = hidden_states[:, 0]
                pre = self.bert_model.classifier.pre_linear(pooled)
                act = self.bert_model.classifier.acti(pre)
                drop = self.bert_model.classifier.dropout(act)
                logits = self.bert_model.classifier.linear(drop)
                return logits
            else:  # probs
                return self.bert_model.predict(x_dataset)


def to_class_indices(y: torch.Tensor) -> torch.Tensor:
    """one-hot/soft æ ‡ç­¾ -> ç±»åˆ«ç´¢å¼• (LongTensor)ã€‚"""
    if y.ndim > 1 and y.shape[1] > 1:
        return torch.argmax(y, dim=1).long()
    # å·²æ˜¯ä¸€ç»´, ç¡®ä¿ long
    return y.long().view(-1)


def run_stage_lava(
    dataset: str,
    train_count: int,
    valid_count: int,
    test_count: int,
    seed: int,
    pretrained_model_name: str,
    device: torch.device,
    stage_epochs: int,
    finetune_batch_size: int,
    finetune_lr: float,
    embedding_mode: str,
) -> np.ndarray:
    """åœ¨æŒ‡å®š stage (t=stage_epochs) ä¸‹è¿è¡Œä¸€æ¬¡ LAVA, è¿”å› data valuesã€‚"""
    # åŠ è½½æ•°æ® (ä¸åŠ å™ªå£°)
    fetcher = DataFetcher.setup(
        dataset_name=dataset,
        train_count=train_count,
        valid_count=valid_count,
        test_count=test_count,
        random_state=seed,
        add_noise=None,
    )
    x_train, y_train, x_valid, y_valid, *_ = fetcher.datapoints

    # æ„å»º BERT æ¨¡å‹
    num_classes = fetcher.label_dim[0]
    bert = BertClassifier(
        pretrained_model_name=pretrained_model_name, num_classes=num_classes
    ).to(device)

    # æŒ‰ t è¿›è¡Œå¾®è°ƒ (t=0åˆ™è·³è¿‡)
    if stage_epochs > 0:
        y_train_idx = to_class_indices(y_train)
        bert.fit(
            x_train,
            y_train_idx,
            batch_size=finetune_batch_size,
            epochs=stage_epochs,
            lr=finetune_lr,
        )

    # åŒ…è£…ç”¨äº LAVA çš„åµŒå…¥æ¨¡å‹
    emb_model = BertEmbeddingWrapper(bert, mode=embedding_mode)

    lava = LavaEvaluator(device=device, embedding_model=emb_model, random_state=seed)
    lava.input_data(x_train, y_train, x_valid, y_valid)
    lava.train_data_values()
    dv = lava.evaluate_data_values()
    return dv


def parse_stages(arg: str, T: int) -> List[int]:
    if arg == "all":
        return list(range(T + 1))
    parts = [p.strip() for p in arg.split(",") if p.strip()]
    stages: List[int] = []
    for p in parts:
        try:
            v = int(p)
        except Exception as e:
            raise ValueError(f"Invalid stage '{p}', must be int or 'all'") from e
        if v < 0 or v > T:
            raise ValueError(f"Stage {v} out of range [0, {T}]")
        stages.append(v)
    return sorted(set(stages))


def main():
    parser = argparse.ArgumentParser(description="LAVA + BERT Sentiment CLI")
    # æ•°æ®é›†ä¸è§„æ¨¡
    parser.add_argument(
        "--dataset", default="imdb", help="dataset name (default: imdb)"
    )
    parser.add_argument("--train-count", type=int, default=16384)
    parser.add_argument("--valid-count", type=int, default=2048)
    parser.add_argument("--test-count", type=int, default=2048)

    # è®­ç»ƒ/é˜¶æ®µ
    parser.add_argument(
        "--epochs", type=int, default=10, help="T: max fine-tune epochs"
    )
    parser.add_argument(
        "--stages",
        default="all",
        help="stage list like '0,2,5' or 'all' for [0..T]",
    )
    parser.add_argument(
        "--batch-size", type=int, default=8, help="fine-tune batch size"
    )
    parser.add_argument(
        "--lr", type=float, default=2e-5, help="fine-tune learning rate"
    )

    # æ¨¡å‹ä¸è®¾å¤‡
    parser.add_argument(
        "--pretrained-model",
        default="distilbert-base-uncased",
        help="HF pretrained model id for DistilBERT",
    )
    parser.add_argument(
        "--embedding-mode",
        choices=["pooled", "logits", "probs"],
        default="pooled",
        help="feature type for LAVA (default: pooled)",
    )
    parser.add_argument(
        "--device",
        choices=["auto", "cuda", "mps", "cpu"],
        default="auto",
        help="device preference",
    )
    parser.add_argument(
        "--gpu",
        type=int,
        default=None,
        help="GPU id to use when CUDA is available (e.g., --gpu 2)",
    )
    parser.add_argument("--seed", type=int, default=42)

    # è¾“å‡º
    parser.add_argument(
        "--output-dir",
        default="results/lava_bert_cli",
        help="directory to save outputs",
    )
    parser.add_argument(
        "--save-prefix",
        default="lava",
        help="filename prefix for saved arrays",
    )

    args = parser.parse_args()

    device = select_device(args.device, args.gpu)
    # If CUDA with explicit GPU id, set it
    if device.type == "cuda" and args.gpu is not None:
        try:
            torch.cuda.set_device(device)
        except Exception:
            # Fallback: ignore if cannot set
            pass
    set_seed(args.seed)

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    stages = parse_stages(args.stages, args.epochs)

    print("ğŸ”§ Config:")
    print(f"  dataset         : {args.dataset}")
    print(
        f"  splits          : train={args.train_count}, valid={args.valid_count}, test={args.test_count}"
    )
    print(f"  epochs (T)      : {args.epochs}")
    print(f"  stages          : {stages}")
    print(f"  lr, batch_size  : {args.lr}, {args.batch_size}")
    print(f"  pretrained      : {args.pretrained_model}")
    print(f"  embedding_mode  : {args.embedding_mode}")
    gpu_descr = f", gpu={args.gpu}" if (device.type == "cuda" and args.gpu is not None) else ""
    print(f"  device, seed    : {device}{gpu_descr}, {args.seed}")
    print(f"  output_dir      : {out_dir}")

    all_stats = {}

    for t in stages:
        print("=" * 60)
        print(f"ğŸš€ Stage [0, {t}] â†’ fine-tune epochs = {t}")
        dv = run_stage_lava(
            dataset=args.dataset,
            train_count=args.train_count,
            valid_count=args.valid_count,
            test_count=args.test_count,
            seed=args.seed,
            pretrained_model_name=args.pretrained_model,
            device=device,
            stage_epochs=t,
            finetune_batch_size=args.batch_size,
            finetune_lr=args.lr,
            embedding_mode=args.embedding_mode,
        )

        arr = np.asarray(dv, dtype=np.float64)
        stats = {
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr)),
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
            "count": int(arr.size),
        }
        all_stats[f"stage_{t}"] = stats

        npy_path = (
            out_dir / f"{args.save_prefix}_{args.dataset}_t{t}_seed{args.seed}.npy"
        )
        np.save(npy_path, arr)
        print(f"âœ… Saved stage {t} data values â†’ {npy_path}")
        print(f"   stats: {stats}")

    summary = {
        "dataset": args.dataset,
        "train_count": args.train_count,
        "valid_count": args.valid_count,
        "test_count": args.test_count,
        "epochs": args.epochs,
        "lr": args.lr,
        "batch_size": args.batch_size,
        "pretrained_model": args.pretrained_model,
        "embedding_mode": args.embedding_mode,
        "device": str(device),
        "seed": args.seed,
        "stages": stages,
        "stats": all_stats,
    }

    summary_path = (
        out_dir / f"{args.save_prefix}_{args.dataset}_summary_seed{args.seed}.json"
    )
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ Summary saved â†’ {summary_path}")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
LAVA + BERT æƒ…æ„Ÿåˆ†æ CLI

åŠŸèƒ½
----
- è¿è¡Œ LAVA æ•°æ®ä»·å€¼è¯„ä¼°ï¼Œä½¿ç”¨ BERT ä½œä¸ºåµŒå…¥æ¨¡å‹ï¼ˆé»˜è®¤è¾“å‡ºCLS pooled embeddingï¼‰ã€‚
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è°ƒæ•´æ•°æ®è§„æ¨¡ã€epochã€å­¦ä¹ ç‡ã€batch sizeã€è®¾å¤‡ç­‰ã€‚
- æ”¯æŒæŒ‰é˜¶æ®µ [0, t] (t <= T) è®¡ç®—å½±å“åŠ›ï¼š
  å¯¹æ¯ä¸ª tï¼Œä½¿ç”¨ t ä½œä¸º fine-tune çš„æ€» epoch æ•°æ¥è®­ç»ƒ/å¾®è°ƒ BERTï¼Œç„¶åè®¡ç®—ä¸€æ¬¡ LAVAã€‚

æ³¨æ„
----
- LAVA è‡ªèº«ä¸ä¾èµ–æ—¶é—´çª—å£ï¼›è¿™é‡Œçš„é˜¶æ®µæ˜¯é€šè¿‡â€œå°† fine-tune è½®æ•°è®¾ä¸º tâ€æ¥å¾—åˆ°ä¸åŒé˜¶æ®µçš„åµŒå…¥ï¼Œå†è®¡ç®— LAVAã€‚
- è‹¥ t=0ï¼Œåˆ™ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒ BERTï¼ˆä¸å¾®è°ƒï¼‰ã€‚
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Iterable, List, Optional

import numpy as np
import torch

from opendataval.dataloader import DataFetcher
from opendataval.dataval.lava import LavaEvaluator
from opendataval.model import BertClassifier


def select_device(pref: str = "auto") -> torch.device:
    if pref == "cuda" and torch.cuda.is_available():
        return torch.device("cuda")
    if pref == "mps" and torch.backends.mps.is_available():
        return torch.device("mps")
    if pref == "cpu":
        return torch.device("cpu")
    # auto
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def set_seed(seed: int):
    torch.manual_seed(seed)
    np.random.seed(seed)


class BertEmbeddingWrapper(torch.nn.Module):
    """åŒ…è£… BertClassifierï¼Œä½¿å…¶ predict è¿”å›å¯ç”¨ä½œ LAVA ç‰¹å¾çš„å‘é‡ã€‚

    æ¨¡å¼ï¼š
    - pooled: è¿”å› CLS pooled embeddingï¼ˆé¦– token å¯¹åº”çš„éšè—çŠ¶æ€ï¼‰ã€‚
    - logits: è¿”å›åˆ†ç±»å™¨çº¿æ€§å±‚è¾“å‡ºï¼ˆæœª softmax çš„ logitsï¼‰ã€‚
    - probs: è¿”å›åˆ†ç±»æ¦‚ç‡ï¼ˆBertClassifier.predict çš„é»˜è®¤è¾“å‡ºï¼‰ã€‚
    """

    def __init__(self, bert_model: BertClassifier, mode: str = "pooled"):
        super().__init__()
        assert mode in {"pooled", "logits", "probs"}
        self.bert_model = bert_model
        self.mode = mode

    @property
    def device(self) -> torch.device:
        return self.bert_model.bert.device

    def predict(self, x_dataset) -> torch.Tensor:
        # x_dataset æ˜¯ ListDataset[str] æˆ–å…¼å®¹çš„ Dataset
        # ç»Ÿä¸€èµ° tokenizationï¼Œç„¶åæŒ‰éœ€è¦è¿”å›ç‰¹å¾
        if len(x_dataset) == 0:
            return torch.zeros(0, 1, device=self.device)

        # å°†æ–‡æœ¬è½¬ä¸º input_ids & attention_mask
        bert_inputs = self.bert_model.tokenize(x_dataset)
        input_ids, attention_mask = [t.to(self.device) for t in bert_inputs.tensors]

        with torch.no_grad():
            if self.mode == "pooled":
                hidden_states = self.bert_model.bert(
                    input_ids=input_ids, attention_mask=attention_mask
                )[0]
                pooled = hidden_states[:, 0]
                return pooled
            elif self.mode == "logits":
                # å¤ç”¨åˆ†ç±»å™¨ï¼Œä½†æˆªæ–­ softmaxï¼šåªå–çº¿æ€§å±‚ logits
                hidden_states = self.bert_model.bert(
                    input_ids=input_ids, attention_mask=attention_mask
                )[0]
                pooled = hidden_states[:, 0]
                pre = self.bert_model.classifier.pre_linear(pooled)
                act = self.bert_model.classifier.acti(pre)
                drop = self.bert_model.classifier.dropout(act)
                logits = self.bert_model.classifier.linear(drop)
                return logits
            else:  # probs
                return self.bert_model.predict(x_dataset)


def to_class_indices(y: torch.Tensor) -> torch.Tensor:
    """one-hot/soft æ ‡ç­¾ -> ç±»åˆ«ç´¢å¼• (LongTensor)ã€‚"""
    if y.ndim > 1 and y.shape[1] > 1:
        return torch.argmax(y, dim=1).long()
    # å·²æ˜¯ä¸€ç»´ï¼Œç¡®ä¿ long
    return y.long().view(-1)


def run_stage_lava(
    dataset: str,
    train_count: int,
    valid_count: int,
    test_count: int,
    seed: int,
    pretrained_model_name: str,
    device: torch.device,
    stage_epochs: int,
    finetune_batch_size: int,
    finetune_lr: float,
    embedding_mode: str,
) -> np.ndarray:
    """åœ¨æŒ‡å®š stageï¼ˆt=stage_epochsï¼‰ä¸‹è¿è¡Œä¸€æ¬¡ LAVAï¼Œè¿”å› data valuesã€‚"""
    # åŠ è½½æ•°æ®ï¼ˆä¸åŠ å™ªå£°ï¼‰
    fetcher = DataFetcher.setup(
        dataset_name=dataset,
        train_count=train_count,
        valid_count=valid_count,
        test_count=test_count,
        random_state=seed,
        add_noise=None,
    )
    x_train, y_train, x_valid, y_valid, *_ = fetcher.datapoints

    # æ„å»º BERT æ¨¡å‹
    num_classes = fetcher.label_dim[0]
    bert = BertClassifier(
        pretrained_model_name=pretrained_model_name, num_classes=num_classes
    ).to(device)

    # æŒ‰ t è¿›è¡Œå¾®è°ƒï¼ˆt=0åˆ™è·³è¿‡ï¼‰
    if stage_epochs > 0:
        y_train_idx = to_class_indices(y_train)
        bert.fit(
            x_train,
            y_train_idx,
            batch_size=finetune_batch_size,
            epochs=stage_epochs,
            lr=finetune_lr,
        )

    # åŒ…è£…ç”¨äº LAVA çš„åµŒå…¥æ¨¡å‹
    emb_model = BertEmbeddingWrapper(bert, mode=embedding_mode)

    lava = LavaEvaluator(device=device, embedding_model=emb_model, random_state=seed)
    lava.input_data(x_train, y_train, x_valid, y_valid)
    lava.train_data_values()
    dv = lava.evaluate_data_values()
    return dv


def parse_stages(arg: str, T: int) -> List[int]:
    if arg == "all":
        return list(range(T + 1))
    parts = [p.strip() for p in arg.split(",") if p.strip()]
    stages: List[int] = []
    for p in parts:
        try:
            v = int(p)
        except Exception as e:
            raise ValueError(f"Invalid stage '{p}', must be int or 'all'") from e
        if v < 0 or v > T:
            raise ValueError(f"Stage {v} out of range [0, {T}]")
        stages.append(v)
    return sorted(set(stages))


def main():
    parser = argparse.ArgumentParser(description="LAVA + BERT Sentiment CLI")
    # æ•°æ®é›†ä¸è§„æ¨¡
    parser.add_argument("--dataset", default="imdb", help="dataset name (default: imdb)")
    parser.add_argument("--train-count", type=int, default=16384)
    parser.add_argument("--valid-count", type=int, default=2048)
    parser.add_argument("--test-count", type=int, default=2048)

    # è®­ç»ƒ/é˜¶æ®µ
    parser.add_argument("--epochs", type=int, default=10, help="T: max fine-tune epochs")
    parser.add_argument(
        "--stages",
        default="all",
        help="stage list like '0,2,5' or 'all' for [0..T]",
    )
    parser.add_argument("--batch-size", type=int, default=8, help="fine-tune batch size")
    parser.add_argument("--lr", type=float, default=2e-5, help="fine-tune learning rate")

    # æ¨¡å‹ä¸è®¾å¤‡
    parser.add_argument(
        "--pretrained-model",
        default="distilbert-base-uncased",
        help="HF pretrained model id for DistilBERT",
    )
    parser.add_argument(
        "--embedding-mode",
        choices=["pooled", "logits", "probs"],
        default="pooled",
        help="feature type for LAVA (default: pooled)")
    parser.add_argument(
        "--device",
        choices=["auto", "cuda", "mps", "cpu"],
        default="auto",
        help="device preference",
    )
    parser.add_argument("--seed", type=int, default=42)

    # è¾“å‡º
    parser.add_argument(
        "--output-dir", default="results/lava_bert_cli", help="directory to save outputs"
    )
    parser.add_argument(
        "--save-prefix",
        default="lava",
        help="filename prefix for saved arrays",
    )

    args = parser.parse_args()

    device = select_device(args.device)
    set_seed(args.seed)

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    stages = parse_stages(args.stages, args.epochs)

    print("ğŸ”§ Config:")
    print(f"  dataset         : {args.dataset}")
    print(f"  splits          : train={args.train_count}, valid={args.valid_count}, test={args.test_count}")
    print(f"  epochs (T)      : {args.epochs}")
    print(f"  stages          : {stages}")
    print(f"  lr, batch_size  : {args.lr}, {args.batch_size}")
    print(f"  pretrained      : {args.pretrained_model}")
    print(f"  embedding_mode  : {args.embedding_mode}")
    print(f"  device, seed    : {device}, {args.seed}")
    print(f"  output_dir      : {out_dir}")

    all_stats = {}

    for t in stages:
        print("=" * 60)
        print(f"ğŸš€ Stage [0, {t}] â†’ fine-tune epochs = {t}")
        dv = run_stage_lava(
            dataset=args.dataset,
            train_count=args.train_count,
            valid_count=args.valid_count,
            test_count=args.test_count,
            seed=args.seed,
            pretrained_model_name=args.pretrained_model,
            device=device,
            stage_epochs=t,
            finetune_batch_size=args.batch_size,
            finetune_lr=args.lr,
            embedding_mode=args.embedding_mode,
        )

        arr = np.asarray(dv, dtype=np.float64)
        stats = {
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr)),
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
            "count": int(arr.size),
        }
        all_stats[f"stage_{t}"] = stats

        npy_path = out_dir / f"{args.save_prefix}_{args.dataset}_t{t}_seed{args.seed}.npy"
        np.save(npy_path, arr)
        print(f"âœ… Saved stage {t} data values â†’ {npy_path}")
        print(f"   stats: {stats}")

    summary = {
        "dataset": args.dataset,
        "train_count": args.train_count,
        "valid_count": args.valid_count,
        "test_count": args.test_count,
        "epochs": args.epochs,
        "lr": args.lr,
        "batch_size": args.batch_size,
        "pretrained_model": args.pretrained_model,
        "embedding_mode": args.embedding_mode,
        "device": str(device),
        "seed": args.seed,
        "stages": stages,
        "stats": all_stats,
    }

    summary_path = out_dir / f"{args.save_prefix}_{args.dataset}_summary_seed{args.seed}.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ Summary saved â†’ {summary_path}")


if __name__ == "__main__":
    main()


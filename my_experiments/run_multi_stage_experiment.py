#!/usr/bin/env python3
"""
Quick runner for multi-stage TIM influence data pruning experiment

This script runs the complete 5-stage experiment with optimized settings for
demonstration and testing purposes.
"""

import sys
import time
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from my_experiments.multi_stage_pruning_experiment import create_multi_stage_experiment


def _print_intro():
    """ÊâìÂç∞ÂÆûÈ™å‰ªãÁªç."""
    print("üöÄ Multi-Stage TIM Influence Data Pruning Experiment")
    print("=" * 80)
    print(
        "This experiment divides training into 5 time windows based on TRAINING STEPS:"
    )
    print("Stage 1: [0, t1] - Early training steps")
    print("Stage 2: [t1, t2] - Initial convergence steps")
    print("Stage 3: [t2, t3] - Mid training steps")
    print("Stage 4: [t3, t4] - Late training steps")
    print("Stage 5: [t4, T] - Final convergence steps")
    print("\nIMPORTANT: Time windows are based on training STEPS, not epochs!")
    print("Each step corresponds to one batch gradient update.")
    print("\nFor each stage, we:")
    print("1. Compute TIM influence scores for that time window")
    print("2. Prune low-influence samples")
    print("3. Retrain on cleaned data")
    print("4. Compare performance across all stages")
    print("=" * 80)


def _create_demo_experiment():
    """ÂàõÂª∫ÊºîÁ§∫ÈÖçÁΩÆÁöÑÂ§öÈò∂ÊÆµÂÆûÈ™åÂØπË±°."""
    return create_multi_stage_experiment(
        dataset_name="imdb",
        train_count=1000,
        valid_count=200,
        test_count=200,
        noise_rate=0.3,
        model_name="distilbert-base-uncased",
        epochs=10,
        batch_size=16,
        num_stages=5,
        output_dir="./multi_stage_demo_results",
        random_state=42,
    )


def _print_config(experiment):
    """ÊâìÂç∞ÂÖ≥ÈîÆÈÖçÁΩÆÂèÇÊï∞."""
    print("\n‚öôÔ∏è  EXPERIMENT CONFIGURATION:")
    print(f"   Dataset: {experiment.config['dataset_name'].upper()}")
    print(f"   Training samples: {experiment.config['train_count']:,}")
    print(f"   Noise rate: {experiment.config['noise_rate']*100:.1f}%")
    print(f"   Total epochs: {experiment.config['epochs']}")
    print(f"   Time windows: {experiment.config['num_stages']}")
    print(f"   Pruning ratio: {experiment.config['prune_ratio']*100:.1f}%")
    print(f"   Output directory: {experiment.config['output_dir']}")


def _print_time_window_breakdown(experiment):
    """ÊåâÊ≠•È™§Áª¥Â∫¶ÊâìÂç∞Êó∂Èó¥Á™óÂè£ÊãÜÂàÜ‰ø°ÊÅØ."""
    print("\n‚è∞ TIME WINDOW BREAKDOWN (STEPS):")
    batch_size = experiment.config["batch_size"]
    train_count = experiment.config["train_count"]
    steps_per_epoch = (train_count + batch_size - 1) // batch_size
    for i, (start, end) in enumerate(experiment.time_windows, 1):
        end_desc = "T" if end is None else str(end)
        total_steps = experiment.config["epochs"] * steps_per_epoch
        steps_in_window = (total_steps - start) if end is None else (end - start + 1)
        epochs_equiv = steps_in_window / steps_per_epoch
        print(
            f"   Stage {i}: [{start}, {end_desc}] - {steps_in_window} steps (~{epochs_equiv:.1f} epochs)"
        )


def _print_overview_and_outputs():
    """ÊâìÂç∞Ê¶ÇËßà‰∏éÈ¢ÑÊúüËæìÂá∫."""
    print("\nüîç EXPERIMENT OVERVIEW:")
    print("This multi-stage experiment will:")
    print("‚Ä¢ Train an original model on noisy data")
    print("‚Ä¢ For each of 5 time windows, compute TIM influence scores")
    print("‚Ä¢ Prune low-influence samples (likely noise) from each stage")
    print("‚Ä¢ Retrain models on cleaned data")
    print("‚Ä¢ Generate comprehensive visualizations comparing all stages")
    print("‚Ä¢ Create performance analysis across different time periods")
    print("\nüìä EXPECTED OUTPUTS:")
    print("‚Ä¢ Multi-stage influence score comparisons")
    print("‚Ä¢ Performance metrics across time windows")
    print("‚Ä¢ Noise detection effectiveness analysis")
    print("‚Ä¢ Training curves for original vs pruned models")
    print("‚Ä¢ Comprehensive experiment dashboard")
    print("‚Ä¢ Time window division diagram")


def _run_experiment_and_report(experiment) -> bool:
    """ËøêË°åÂÆûÈ™åÂπ∂ËæìÂá∫Ê±áÊÄªÊä•Âëä."""
    try:
        start_time = time.time()
        print("\nüß™ Starting experiment...")
        print("This may take several minutes depending on your hardware.")
        print("Progress will be shown for each stage.\n")
        results = experiment.run_complete_experiment()
        total_time = time.time() - start_time
        print("\nüéâ EXPERIMENT COMPLETED!")
        print("=" * 80)
        print(
            f"‚è±Ô∏è  Total Runtime: {total_time:.1f} seconds ({total_time/60:.1f} minutes)"
        )
        print(f"üìÅ Results saved to: {experiment.output_dir}")
        print(f"üé® Plots saved to: {experiment.output_dir / 'plots'}")
        print(f"üéØ Status: {results.get('status', 'unknown').upper()}")
        if results.get("status") == "success":
            successful_stages = [
                s
                for s in results.get("stage_results", {}).values()
                if s["status"] == "success"
            ]
            print("\nüìà KEY RESULTS:")
            print(f"   Successful stages: {len(successful_stages)}/5")
            if successful_stages:
                best_valid_acc = 0
                best_stage = None
                best_time_window = None
                for stage in successful_stages:
                    final_perf = stage["training_results"]["pruned_history"].get(
                        "final_performance", {}
                    )
                    valid_acc = final_perf.get("valid_accuracy", 0)
                    if valid_acc > best_valid_acc:
                        best_valid_acc = valid_acc
                        best_stage = stage["stage_name"]
                        best_time_window = stage["time_window"]["description"]
                if best_stage:
                    print(f"   Best performing stage: {best_stage} {best_time_window}")
                    print(f"   Best validation accuracy: {best_valid_acc:.3f}")
                orig_perf = (
                    results.get("original_training", {})
                    .get("history", {})
                    .get("final_performance", {})
                )
                orig_valid_acc = orig_perf.get("valid_accuracy", 0)
                if orig_valid_acc > 0:
                    improvement = best_valid_acc - orig_valid_acc
                    print(f"   Original model accuracy: {orig_valid_acc:.3f}")
                    print(f"   Improvement: {improvement:+.3f}")
        print("\nüìã GENERATED FILES:")
        print("   ‚Ä¢ multi_stage_experiment_results.json - Complete results")
        print("   ‚Ä¢ experiment_config.json - Configuration used")
        print("   ‚Ä¢ plots/multi_stage_influence_comparison.png")
        print("   ‚Ä¢ plots/stage_performance_comparison.png")
        print("   ‚Ä¢ plots/training_curves_comparison.png")
        print("   ‚Ä¢ plots/influence_heatmap.png")
        print("   ‚Ä¢ plots/time_window_diagram.png")
        print("   ‚Ä¢ plots/comprehensive_dashboard.png")
        print("\n‚úÖ Multi-stage experiment demonstration complete!")
        return True
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Experiment interrupted by user")
        return False
    except Exception as e:
        print(f"\n‚ùå Experiment failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Run multi-stage experiment with demo settings."""
    _print_intro()
    experiment = _create_demo_experiment()
    _print_config(experiment)
    _print_time_window_breakdown(experiment)
    _print_overview_and_outputs()
    return _run_experiment_and_report(experiment)


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

#!/usr/bin/env python3
"""
è°ƒè¯•æ¢¯åº¦é—®é¢˜
"""
import torch
from bert_sentiment_analysis import BertTimExperiment


def debug_gradient_issue():
    """è°ƒè¯•æ¢¯åº¦é—®é¢˜"""
    print("ğŸ” è°ƒè¯•æ¢¯åº¦é—®é¢˜")

    experiment = BertTimExperiment(
        dataset_name="imdb", train_count=5, valid_count=3, test_count=2, random_state=42
    )

    # å‡†å¤‡æ•°æ®
    data = experiment.prepare_data()
    x_train, y_train, x_valid, y_valid, x_test, y_test = data

    # åˆ›å»ºæ¨¡å‹ - å¼ºåˆ¶ä½¿ç”¨CPUé¿å…MPSé—®é¢˜
    from bert_sentiment_analysis import get_bert_model_configs

    from opendataval.model import BertClassifier

    configs = get_bert_model_configs()
    model_config = configs["distilbert-base-uncased"]

    model = BertClassifier(
        pretrained_model_name=model_config["pretrained_model_name"],
        num_classes=2,
        dropout_rate=0.2,
        num_train_layers=2,
    )

    # å¼ºåˆ¶ä½¿ç”¨CPU
    device = torch.device("cpu")
    model = model.to(device)
    print(f"ğŸ¤– æ¨¡å‹è®¾å¤‡: {device}")

    print("ğŸ§ª æµ‹è¯•BERTæ¨¡å‹æ¢¯åº¦...")

    # æµ‹è¯•åŸºæœ¬BERTæ¨¡å‹æ¢¯åº¦
    model.train()

    # Tokenizeæ•°æ®
    train_dataset = model.tokenize(x_train)
    train_input_ids = train_dataset.tensors[0]
    train_attention_mask = train_dataset.tensors[1]

    print(f"Input IDs shape: {train_input_ids.shape}")
    print(f"Input IDs requires_grad: {train_input_ids.requires_grad}")

    # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    print("\nğŸ”„ æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    with torch.enable_grad():
        outputs = model(train_input_ids[:2], train_attention_mask[:2])
        print(f"Outputs shape: {outputs.shape}")
        print(f"Outputs requires_grad: {outputs.requires_grad}")

        # è®¡ç®—æŸå¤±
        targets = y_train[:2]
        print(f"Targets shape: {targets.shape}")
        print(f"Targets: {targets}")

        # å¦‚æœæ˜¯one-hotç¼–ç , è½¬æ¢ä¸ºç´¢å¼•
        if len(targets.shape) > 1 and targets.shape[1] > 1:
            # one-hot to indices
            target_indices = torch.argmax(targets, dim=1)
            print(f"Target indices: {target_indices}")
            loss = torch.nn.functional.cross_entropy(outputs, target_indices)
        else:
            loss = torch.nn.functional.mse_loss(outputs, targets.float())

        print(f"Loss: {loss}")
        print(f"Loss requires_grad: {loss.requires_grad}")

        # æµ‹è¯•åå‘ä¼ æ’­
        print("\nğŸ”„ æµ‹è¯•åå‘ä¼ æ’­...")
        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        grad_count = 0
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_count += 1
                print(f"Parameter {name}: grad_norm = {param.grad.norm().item():.6f}")
            else:
                print(f"Parameter {name}: NO GRADIENT")

        print(f"\nâœ… æ€»å…± {grad_count} ä¸ªå‚æ•°æœ‰æ¢¯åº¦")

        if grad_count > 0:
            print("ğŸ‰ BERTæ¨¡å‹æ¢¯åº¦æ­£å¸¸! ")
            return True
        else:
            print("âŒ BERTæ¨¡å‹æ²¡æœ‰æ¢¯åº¦")
            return False


if __name__ == "__main__":
    debug_gradient_issue()

"""
Enhanced Multi-Stage Visualization Module

Provides comprehensive visualization functions for multi-stage BERT training process
and TIM influence analysis, with support for time window comparisons, stage-wise
performance tracking, and comparative analysis across different time periods.
All text labels and outputs are in English.
"""

import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np

try:
    import seaborn as sns

    HAS_SEABORN = True
    # Set font and styling
    plt.rcParams["font.sans-serif"] = ["Arial", "DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    sns.set_style("whitegrid")
    plt.rcParams["figure.facecolor"] = "white"
except ImportError:
    HAS_SEABORN = False
    print("âš ï¸ seaborn not found, using basic matplotlib styling")
    # Set basic matplotlib styling
    plt.rcParams["font.sans-serif"] = ["Arial", "DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    plt.rcParams["figure.facecolor"] = "white"
    plt.style.use("default")


class MultiStageVisualizer:
    """Enhanced Multi-Stage Experiment Visualizer"""

    def __init__(
        self,
        save_dir: str = "./multi_stage_plots",
        figure_size: Tuple[int, int] = (12, 8),
        dpi: int = 150,
    ):
        """
        Initialize multi-stage visualizer

        Parameters:
        -----------
        save_dir : str
            Directory to save plots
        figure_size : Tuple[int, int]
            Figure size
        dpi : int
            Figure DPI
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.figure_size = figure_size
        self.dpi = dpi

        # Enhanced color scheme for multi-stage visualization
        self.colors = {
            "primary": "#2E86C1",
            "secondary": "#28B463",
            "accent": "#F39C12",
            "warning": "#E74C3C",
            "neutral": "#85929E",
            "noise": "#E74C3C",
            "clean": "#28B463",
            "original": "#2E86C1",
            "pruned": "#8E44AD",
            # Stage-specific colors
            "stage_1": "#1f77b4",
            "stage_2": "#ff7f0e",
            "stage_3": "#2ca02c",
            "stage_4": "#d62728",
            "stage_5": "#9467bd",
            "stage_colors": [
                "#1f77b4",
                "#ff7f0e",
                "#2ca02c",
                "#d62728",
                "#9467bd",
                "#8c564b",
            ],
        }

    def plot_multi_stage_influence_comparison(
        self,
        stage_results: List[Dict],
        title: str = "TIM Influence Scores Across Time Windows",
        save_name: str = "multi_stage_influence_comparison.png",
    ):
        """
        Plot influence score distributions across multiple stages

        Parameters:
        -----------
        stage_results : List[Dict]
            List of stage results containing influence analysis
        title : str
            Plot title
        save_name : str
            Save filename
        """
        print(f"Creating multi-stage influence comparison: {title}")

        num_stages = len(stage_results)
        cols = min(3, num_stages)
        rows = (num_stages + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 4 * rows))
        fig.suptitle(title, fontsize=16, fontweight="bold")

        if num_stages == 1:
            axes = [axes]
        elif rows == 1:
            axes = [axes] if num_stages == 1 else list(axes)
        else:
            axes = axes.flatten()

        for i, stage_result in enumerate(stage_results):
            if i >= len(axes):
                break

            ax = axes[i]

            influence_scores = np.array(stage_result["influence_analysis"]["scores"])
            stage_name = stage_result["stage_name"]
            time_desc = stage_result["time_window"]["description"]

            # Histogram of influence scores
            color = self.colors["stage_colors"][i % len(self.colors["stage_colors"])]
            ax.hist(
                influence_scores, bins=30, alpha=0.7, color=color, edgecolor="black"
            )
            ax.axvline(
                np.mean(influence_scores),
                color=self.colors["warning"],
                linestyle="--",
                linewidth=2,
                label=f"Mean: {np.mean(influence_scores):.6f}",
            )

            ax.set_title(f"{stage_name}\nTime Window: {time_desc}")
            ax.set_xlabel("Influence Score")
            ax.set_ylabel("Frequency")
            ax.legend()
            ax.grid(True, alpha=0.3)

        # Remove empty subplots
        for i in range(num_stages, len(axes)):
            fig.delaxes(axes[i])

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight")
        plt.show()
        print(f"Multi-stage influence comparison saved: {save_path}")

    def plot_stage_performance_comparison(
        self,
        stage_results: List[Dict],
        original_history: Optional[Dict] = None,
        title: str = "Performance Comparison Across Time Windows",
        save_name: str = "stage_performance_comparison.png",
    ):
        """
        Plot performance comparison across stages

        Parameters:
        -----------
        stage_results : List[Dict]
            List of stage results
        original_history : Dict
            Original model training history for comparison
        title : str
            Plot title
        save_name : str
            Save filename
        """
        print(f"Creating stage performance comparison: {title}")

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(title, fontsize=16, fontweight="bold")

        # Extract data
        stage_names = [s["stage_name"] for s in stage_results]
        time_windows = [s["time_window"]["description"] for s in stage_results]

        # Performance metrics
        final_train_acc = []
        final_valid_acc = []
        final_train_loss = []
        noise_recall = []
        noise_precision = []
        mean_influence = []

        for stage in stage_results:
            hist = stage["training_results"]["pruned_history"]
            final_perf = hist.get("final_performance", {})

            final_train_acc.append(final_perf.get("train_accuracy", 0))
            final_valid_acc.append(final_perf.get("valid_accuracy", 0))
            final_train_loss.append(final_perf.get("train_loss", 0))

            noise_det = stage["pruning_analysis"]["noise_detection"]
            noise_recall.append(noise_det["noise_recall"])
            noise_precision.append(noise_det["noise_precision"])

            inf_stats = stage["influence_analysis"]
            mean_influence.append(inf_stats["mean_influence"])

        # 1. Training accuracy comparison
        x = np.arange(len(stage_names))
        colors = [
            self.colors["stage_colors"][i % len(self.colors["stage_colors"])]
            for i in range(len(stage_names))
        ]

        bars1 = ax1.bar(x, final_train_acc, color=colors, alpha=0.7, edgecolor="black")
        ax1.set_title("Final Training Accuracy by Stage")
        ax1.set_xlabel("Time Window Stage")
        ax1.set_ylabel("Training Accuracy")
        ax1.set_xticks(x)
        ax1.set_xticklabels(
            [f"{name}\n{win}" for name, win in zip(stage_names, time_windows)],
            rotation=45,
        )
        ax1.grid(True, alpha=0.3)

        # Add original model baseline if available
        if original_history and "final_performance" in original_history:
            orig_acc = original_history["final_performance"].get("train_accuracy", 0)
            ax1.axhline(
                orig_acc,
                color=self.colors["original"],
                linestyle="--",
                linewidth=2,
                alpha=0.8,
                label=f"Original (Noisy): {orig_acc:.3f}",
            )
            ax1.legend()

        # Add value labels
        for i, (bar, acc) in enumerate(zip(bars1, final_train_acc)):
            ax1.text(
                bar.get_x() + bar.get_width() / 2,
                acc + 0.01,
                f"{acc:.3f}",
                ha="center",
                va="bottom",
                fontweight="bold",
                fontsize=10,
            )

        # 2. Validation accuracy comparison
        bars2 = ax2.bar(x, final_valid_acc, color=colors, alpha=0.7, edgecolor="black")
        ax2.set_title("Final Validation Accuracy by Stage")
        ax2.set_xlabel("Time Window Stage")
        ax2.set_ylabel("Validation Accuracy")
        ax2.set_xticks(x)
        ax2.set_xticklabels(
            [f"{name}\n{win}" for name, win in zip(stage_names, time_windows)],
            rotation=45,
        )
        ax2.grid(True, alpha=0.3)

        # Add original model baseline if available
        if original_history and "final_performance" in original_history:
            orig_acc = original_history["final_performance"].get("valid_accuracy", 0)
            ax2.axhline(
                orig_acc,
                color=self.colors["original"],
                linestyle="--",
                linewidth=2,
                alpha=0.8,
                label=f"Original (Noisy): {orig_acc:.3f}",
            )
            ax2.legend()

        # Add value labels
        for i, (bar, acc) in enumerate(zip(bars2, final_valid_acc)):
            ax2.text(
                bar.get_x() + bar.get_width() / 2,
                acc + 0.01,
                f"{acc:.3f}",
                ha="center",
                va="bottom",
                fontweight="bold",
                fontsize=10,
            )

        # 3. Noise detection effectiveness (Recall vs Precision)
        ax3.scatter(
            noise_recall, noise_precision, s=150, c=colors, alpha=0.7, edgecolor="black"
        )

        # Add stage labels
        for i, (recall, precision, name) in enumerate(
            zip(noise_recall, noise_precision, stage_names)
        ):
            ax3.annotate(
                name,
                (recall, precision),
                xytext=(5, 5),
                textcoords="offset points",
                fontsize=10,
                fontweight="bold",
            )

        ax3.set_title("Noise Detection: Recall vs Precision")
        ax3.set_xlabel("Noise Recall")
        ax3.set_ylabel("Noise Precision")
        ax3.grid(True, alpha=0.3)
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)

        # Add diagonal reference line
        ax3.plot([0, 1], [0, 1], "k--", alpha=0.3, label="Perfect Balance")
        ax3.legend()

        # 4. Mean influence scores by stage
        bars4 = ax4.bar(x, mean_influence, color=colors, alpha=0.7, edgecolor="black")
        ax4.set_title("Mean Influence Score by Stage")
        ax4.set_xlabel("Time Window Stage")
        ax4.set_ylabel("Mean Influence Score")
        ax4.set_xticks(x)
        ax4.set_xticklabels(
            [f"{name}\n{win}" for name, win in zip(stage_names, time_windows)],
            rotation=45,
        )
        ax4.grid(True, alpha=0.3)

        # Add value labels
        for i, (bar, inf) in enumerate(zip(bars4, mean_influence)):
            ax4.text(
                bar.get_x() + bar.get_width() / 2,
                inf + inf * 0.05,
                f"{inf:.6f}",
                ha="center",
                va="bottom",
                fontweight="bold",
                fontsize=9,
                rotation=90,
            )

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight")
        plt.show()
        print(f"Stage performance comparison saved: {save_path}")

    def plot_training_curves_comparison(
        self,
        original_history: Dict,
        stage_results: List[Dict],
        title: str = "Training Loss Curves: Original vs Multi-Stage Pruned Models",
        save_name: str = "training_curves_comparison.png",
    ):
        """
        Plot training curves comparison between original and all stage models

        Parameters:
        -----------
        original_history : Dict
            Original model training history
        stage_results : List[Dict]
            List of stage results
        title : str
            Plot title
        save_name : str
            Save filename
        """
        print(f"Creating training curves comparison: {title}")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(title, fontsize=16, fontweight="bold")

        # 1. Training loss comparison
        if "train_loss" in original_history:
            epochs = range(1, len(original_history["train_loss"]) + 1)
            ax1.plot(
                epochs,
                original_history["train_loss"],
                color=self.colors["warning"],
                linewidth=3,
                label="Original (Noisy)",
                alpha=0.8,
            )

        # Plot pruned training curves for each stage
        for i, stage in enumerate(stage_results):
            hist = stage["training_results"]["pruned_history"]
            if "train_loss" in hist:
                epochs = range(1, len(hist["train_loss"]) + 1)
                stage_name = stage["stage_name"]
                time_desc = stage["time_window"]["description"]
                color = self.colors["stage_colors"][
                    i % len(self.colors["stage_colors"])
                ]

                ax1.plot(
                    epochs,
                    hist["train_loss"],
                    color=color,
                    linewidth=2,
                    label=f"{stage_name} {time_desc}",
                    alpha=0.7,
                )

        ax1.set_title("Training Loss Comparison")
        ax1.set_xlabel("Epoch")
        ax1.set_ylabel("Training Loss")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Training accuracy comparison
        if "train_accuracy" in original_history:
            epochs = range(1, len(original_history["train_accuracy"]) + 1)
            ax2.plot(
                epochs,
                original_history["train_accuracy"],
                color=self.colors["warning"],
                linewidth=3,
                label="Original (Noisy)",
                alpha=0.8,
            )

        for i, stage in enumerate(stage_results):
            hist = stage["training_results"]["pruned_history"]
            if "train_accuracy" in hist:
                epochs = range(1, len(hist["train_accuracy"]) + 1)
                stage_name = stage["stage_name"]
                time_desc = stage["time_window"]["description"]
                color = self.colors["stage_colors"][
                    i % len(self.colors["stage_colors"])
                ]

                ax2.plot(
                    epochs,
                    hist["train_accuracy"],
                    color=color,
                    linewidth=2,
                    label=f"{stage_name} {time_desc}",
                    alpha=0.7,
                )

        ax2.set_title("Training Accuracy Comparison")
        ax2.set_xlabel("Epoch")
        ax2.set_ylabel("Training Accuracy")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight")
        plt.show()
        print(f"Training curves comparison saved: {save_path}")

    def plot_influence_heatmap(
        self,
        stage_results: List[Dict],
        title: str = "Influence Score Distribution Heatmap Across Time Windows",
        save_name: str = "influence_heatmap.png",
    ):
        """
        Create influence score distribution heatmap across stages

        Parameters:
        -----------
        stage_results : List[Dict]
            List of stage results
        title : str
            Plot title
        save_name : str
            Save filename
        """
        print(f"Creating influence heatmap: {title}")

        fig, ax = plt.subplots(1, 1, figsize=self.figure_size)
        fig.suptitle(title, fontsize=16, fontweight="bold")

        # Create influence matrix
        influence_matrix = []
        stage_names = []

        # Find global influence range for consistent binning
        all_influences = []
        for stage in stage_results:
            influences = np.array(stage["influence_analysis"]["scores"])
            all_influences.extend(influences)

        global_min, global_max = np.min(all_influences), np.max(all_influences)

        for stage in stage_results:
            inf_scores = np.array(stage["influence_analysis"]["scores"])
            stage_names.append(
                f"{stage['stage_name']}\n{stage['time_window']['description']}"
            )

            # Create histogram with consistent bins
            hist, _ = np.histogram(inf_scores, bins=50, range=(global_min, global_max))
            influence_matrix.append(hist)

        if influence_matrix:
            influence_matrix = np.array(influence_matrix)

            # Create heatmap
            im = ax.imshow(
                influence_matrix,
                aspect="auto",
                cmap="viridis",
                interpolation="bilinear",
            )
            ax.set_title("Influence Score Distribution Heatmap")
            ax.set_xlabel("Influence Score Bins (Low â†’ High)")
            ax.set_ylabel("Time Window Stage")
            ax.set_yticks(range(len(stage_names)))
            ax.set_yticklabels(stage_names)

            # Add colorbar
            cbar = plt.colorbar(im, ax=ax, shrink=0.8)
            cbar.set_label("Frequency", rotation=270, labelpad=15)

            # Add influence range annotation
            ax.text(
                0.02,
                0.98,
                f"Influence Range: [{global_min:.6f}, {global_max:.6f}]",
                transform=ax.transAxes,
                fontsize=10,
                verticalalignment="top",
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
            )

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight")
        plt.show()
        print(f"Influence heatmap saved: {save_path}")

    def plot_time_window_diagram(
        self,
        time_windows: List[Tuple[int, Optional[int]]],
        total_steps: int,
        title: str = "Training Time Windows Division (Steps)",
        save_name: str = "time_window_diagram.png",
    ):
        """
        Create time window division diagram

        Parameters:
        -----------
        time_windows : List[Tuple[int, Optional[int]]]
            List of (start_step, end_step) tuples
        total_steps : int
            Total number of training steps
        title : str
            Plot title
        save_name : str
            Save filename
        """
        print(f"Creating time window diagram: {title}")

        fig, ax = plt.subplots(1, 1, figsize=(16, 4))
        fig.suptitle(title, fontsize=16, fontweight="bold")

        # Create timeline
        _ = np.arange(0, total_steps + 1, max(1, total_steps // 100))

        # Draw main timeline
        ax.plot([0, total_steps], [0.5, 0.5], "k-", linewidth=3, alpha=0.3)

        # Mark time windows
        for i, (start, end) in enumerate(time_windows):
            end_step = total_steps if end is None else end
            color = self.colors["stage_colors"][i % len(self.colors["stage_colors"])]

            # Draw window range as a thick line segment
            ax.plot(
                [start, end_step],
                [0.5, 0.5],
                color=color,
                linewidth=12,
                alpha=0.7,
                label=f'Stage {i+1}: [{start}, {"T" if end is None else end}]',
            )

            # Add stage labels
            mid_point = (start + end_step) / 2
            ax.text(
                mid_point,
                0.65,
                f"S{i+1}",
                ha="center",
                va="center",
                fontweight="bold",
                fontsize=14,
                bbox=dict(
                    boxstyle="circle,pad=0.3",
                    facecolor="white",
                    edgecolor=color,
                    linewidth=2,
                ),
            )

            # Add step range labels
            steps_in_window = (
                end_step - start + 1 if end is not None else total_steps - start
            )
            ax.text(
                mid_point,
                0.35,
                f"{steps_in_window} steps",
                ha="center",
                va="center",
                fontsize=10,
                fontweight="bold",
            )

        # Mark step boundaries (show every 10% of total steps)
        step_markers = range(0, total_steps + 1, max(1, total_steps // 10))
        for step in step_markers:
            ax.axvline(step, color="gray", linestyle=":", alpha=0.5)
            ax.text(
                step, 0.2, str(step), ha="center", va="top", fontsize=9, rotation=45
            )

        ax.set_xlabel("Training Step")
        ax.set_xlim(-total_steps * 0.02, total_steps * 1.02)
        ax.set_ylim(0.1, 0.8)
        ax.set_yticks([])
        ax.legend(
            loc="upper center",
            bbox_to_anchor=(0.5, -0.15),
            ncol=min(len(time_windows), 3),
        )
        ax.grid(True, axis="x", alpha=0.3)

        # Add step markers
        ax.text(
            -total_steps * 0.01,
            0.5,
            "0",
            ha="center",
            va="center",
            fontweight="bold",
            fontsize=12,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"),
        )
        ax.text(
            total_steps * 1.01,
            0.5,
            "T",
            ha="center",
            va="center",
            fontweight="bold",
            fontsize=12,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral"),
        )

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight")
        plt.show()
        print(f"Time window diagram saved: {save_path}")

    def plot_comprehensive_dashboard(
        self,
        original_history: Dict,
        stage_results: List[Dict],
        experiment_config: Dict,
        data_stats: Dict,
        title: str = "Multi-Stage TIM Influence Data Pruning Experiment Dashboard",
        save_name: str = "comprehensive_dashboard.png",
    ):
        """
        Create comprehensive experiment dashboard.

        æ³¨é‡Šè¯´æ˜Ž: å°†å„ç»˜å›¾åŒºåŸŸæ‹†åˆ†ä¸ºå°æ­¥éª¤, é™ä½Žåœˆå¤æ‚åº¦.
        """
        print(f"Creating comprehensive dashboard: {title}")

        fig = plt.figure(figsize=(24, 18))
        fig.suptitle(title, fontsize=20, fontweight="bold")
        gs = fig.add_gridspec(5, 6, hspace=0.35, wspace=0.3)

        # 1. è®­ç»ƒæ›²çº¿å¯¹æ¯”
        ax1 = fig.add_subplot(gs[0, :3])
        self._plot_training_loss(ax1, original_history, stage_results)

        # 2. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        ax2 = fig.add_subplot(gs[0, 3:])
        stage_names, colors, final_train_acc, final_valid_acc = (
            self._plot_performance_summary(ax2, original_history, stage_results)
        )

        # 3. å½±å“åŠ›åˆ†å¸ƒçƒ­å›¾
        ax3 = fig.add_subplot(gs[1:3, :3])
        self._plot_influence_heatmap(ax3, stage_results)

        # 4. å™ªå£°æ£€æµ‹æ•ˆæžœ
        ax4 = fig.add_subplot(gs[1, 3:])
        self._plot_noise_effectiveness(ax4, stage_results, stage_names, colors)

        # 5. é˜¶æ®µæ€§èƒ½è¡¨æ ¼
        ax5 = fig.add_subplot(gs[2, 3:])
        self._plot_metrics_table(ax5, stage_results)

        # 6. æ—¶é—´çª—å£ç¤ºæ„
        ax6 = fig.add_subplot(gs[3, :3])
        self._plot_time_window_diagram(ax6, experiment_config, stage_results)

        # 7. ç»Ÿè®¡æ‘˜è¦
        ax7 = fig.add_subplot(gs[3, 3:])
        self._plot_experiment_summary(
            ax7,
            experiment_config,
            data_stats,
            final_train_acc,
            final_valid_acc,
            stage_results,
        )

        # Create performance metrics table
        metrics_data = []
        for i, stage in enumerate(stage_results):
            final_perf = stage["training_results"]["pruned_history"].get(
                "final_performance", {}
            )
            noise_det = stage["pruning_analysis"]["noise_detection"]
            inf_stats = stage["influence_analysis"]

            metrics_data.append(
                [
                    stage["stage_name"],
                    f"{final_perf.get('train_accuracy', 0):.3f}",
                    f"{final_perf.get('valid_accuracy', 0):.3f}",
                    f"{noise_det['noise_recall']:.2%}",
                    f"{noise_det['noise_precision']:.2%}",
                    f"{inf_stats['mean_influence']:.6f}",
                ]
            )

        # Create table
        table_data = metrics_data
        table_headers = [
            "Stage",
            "Train Acc",
            "Valid Acc",
            "Noise Recall",
            "Noise Prec",
            "Mean Influence",
        ]

        table = ax5.table(
            cellText=table_data,
            colLabels=table_headers,
            cellLoc="center",
            loc="center",
            colColours=["lightblue"] * len(table_headers),
        )
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.8)

        ax5.set_title("Performance Metrics Summary", fontsize=14, fontweight="bold")
        ax5.axis("off")

        # 6. Time Window Diagram (bottom left, spans 3 columns)
        ax6 = fig.add_subplot(gs[3, :3])

        total_epochs = experiment_config.get("epochs", 10)
        time_windows = [(0, 2), (2, 4), (4, 6), (6, 8), (8, None)]  # Example windows

        # Create timeline
        timeline = np.arange(total_epochs + 1)
        ax6.plot(timeline, [0.5] * len(timeline), "k-", linewidth=4, alpha=0.4)

        # Mark time windows
        for i, (start, end) in enumerate(time_windows[: len(stage_results)]):
            end_epoch = total_epochs if end is None else end
            window_range = np.arange(start, end_epoch + 1)
            color = self.colors["stage_colors"][i % len(self.colors["stage_colors"])]

            ax6.plot(
                window_range,
                [0.5] * len(window_range),
                color=color,
                linewidth=16,
                alpha=0.8,
                label=f'Stage {i+1}: [{start}, {"T" if end is None else end}]',
            )

            # Add stage labels
            mid_point = (start + end_epoch) / 2
            ax6.text(
                mid_point,
                0.7,
                f"S{i+1}",
                ha="center",
                va="center",
                fontweight="bold",
                fontsize=16,
                bbox=dict(
                    boxstyle="circle,pad=0.5",
                    facecolor="white",
                    edgecolor=color,
                    linewidth=3,
                ),
            )

        ax6.set_title("Training Time Windows Division", fontsize=14, fontweight="bold")
        ax6.set_xlabel("Training Epoch")
        ax6.set_xlim(-0.5, total_epochs + 0.5)
        ax6.set_ylim(0.2, 0.9)
        ax6.set_yticks([])
        ax6.legend(loc="upper center", bbox_to_anchor=(0.5, -0.1), ncol=3)
        ax6.grid(True, axis="x", alpha=0.3)

        # 7. ç»Ÿè®¡æ‘˜è¦å·²åœ¨ä¸Šæ–¹é€šè¿‡ _plot_experiment_summary ç”Ÿæˆ

        # 8. Key Insights and Conclusions (bottom, spans full width)
        ax8 = fig.add_subplot(gs[4, :])

        # Generate insights based on results
        best_stage_idx = np.argmax(final_valid_acc) if final_valid_acc else 0
        best_stage_name = (
            stage_results[best_stage_idx]["stage_name"] if stage_results else "N/A"
        )
        best_time_window = (
            stage_results[best_stage_idx]["time_window"]["description"]
            if stage_results
            else "N/A"
        )

        # è®¡ç®—å™ªå£°æ£€æµ‹æŒ‡æ ‡ç”¨äºŽæ‘˜è¦æ–‡æœ¬
        noise_recall = [
            s["pruning_analysis"]["noise_detection"]["noise_recall"]
            for s in stage_results
        ]
        noise_precision = [
            s["pruning_analysis"]["noise_detection"]["noise_precision"]
            for s in stage_results
        ]

        # è®¡ç®—æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡ç”¨äºŽæ‘˜è¦
        best_valid_acc = max(final_valid_acc) if final_valid_acc else 0

        insights_text = f"""KEY INSIGHTS AND CONCLUSIONS

ðŸ† BEST PERFORMING TIME WINDOW:
{best_stage_name} {best_time_window} achieved the highest validation accuracy ({best_valid_acc:.3f})

ðŸ“Š NOISE DETECTION PATTERNS:
â€¢ Average noise detection recall across stages: {np.mean(noise_recall):.1%}
â€¢ Average noise detection precision across stages: {np.mean(noise_precision):.1%}
â€¢ Most effective stage for noise detection: {stage_names[np.argmax(noise_recall)] if stage_names else 'N/A'} (Recall: {max(noise_recall):.1%})

ðŸ” TIM INFLUENCE ANALYSIS:
â€¢ Influence scores varied significantly across time windows, indicating temporal sensitivity
â€¢ Early vs late training phases showed different noise identification patterns
â€¢ Multi-stage approach provides comprehensive view of sample importance throughout training

ðŸ’¡ PRACTICAL IMPLICATIONS:
â€¢ Different time windows excel at identifying different types of problematic samples
â€¢ Combined multi-stage approach could outperform single-window methods
â€¢ Time-aware data cleaning strategies show promise for improving model robustness"""

        ax8.text(
            0.02,
            0.98,
            insights_text,
            transform=ax8.transAxes,
            fontsize=11,
            verticalalignment="top",
            fontfamily="serif",
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8),
        )
        ax8.set_title("Key Insights and Conclusions", fontsize=16, fontweight="bold")
        ax8.axis("off")

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=self.dpi, bbox_inches="tight", facecolor="white")
        plt.show()
        print(f"Comprehensive dashboard saved: {save_path}")

    # ==== ç§æœ‰ç»˜å›¾è¾…åŠ©æ–¹æ³• ====
    def _plot_training_loss(
        self, ax, original_history: Dict, stage_results: List[Dict]
    ):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±å¯¹æ¯”æ›²çº¿."""
        if "train_loss" in original_history:
            epochs = range(1, len(original_history["train_loss"]) + 1)
            ax.plot(
                epochs,
                original_history["train_loss"],
                color=self.colors["warning"],
                linewidth=4,
                label="Original (Noisy)",
                alpha=0.9,
            )
        for i, stage in enumerate(stage_results):
            hist = stage["training_results"]["pruned_history"]
            if "train_loss" in hist:
                epochs = range(1, len(hist["train_loss"]) + 1)
                color = self.colors["stage_colors"][
                    i % len(self.colors["stage_colors"])
                ]
                ax.plot(
                    epochs,
                    hist["train_loss"],
                    color=color,
                    linewidth=2.5,
                    alpha=0.8,
                    label=f"{stage['stage_name']} {stage['time_window']['description']}",
                )
        ax.set_title(
            "Training Loss: Original vs Multi-Stage Pruned Models",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_xlabel("Epoch")
        ax.set_ylabel("Training Loss")
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_performance_summary(
        self, ax, original_history: Dict, stage_results: List[Dict]
    ):
        """ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½å¯¹æ¯”å¹¶è¿”å›žå…³é”®æ•°æ®."""
        stage_names = [s["stage_name"] for s in stage_results]
        final_train_acc = [
            s["training_results"]["pruned_history"]
            .get("final_performance", {})
            .get("train_accuracy", 0)
            for s in stage_results
        ]
        final_valid_acc = [
            s["training_results"]["pruned_history"]
            .get("final_performance", {})
            .get("valid_accuracy", 0)
            for s in stage_results
        ]
        x = np.arange(len(stage_names))
        width = 0.35
        colors = [
            self.colors["stage_colors"][i % len(self.colors["stage_colors"])]
            for i in range(len(stage_names))
        ]
        bars1 = ax.bar(
            x - width / 2,
            final_train_acc,
            width,
            label="Training Accuracy",
            color=colors,
            alpha=0.7,
            edgecolor="black",
        )
        bars2 = ax.bar(
            x + width / 2,
            final_valid_acc,
            width,
            label="Validation Accuracy",
            color=colors,
            alpha=0.5,
            edgecolor="black",
        )
        if "final_performance" in original_history:
            orig_train = original_history["final_performance"].get("train_accuracy", 0)
            orig_valid = original_history["final_performance"].get("valid_accuracy", 0)
            ax.axhline(
                orig_train,
                color=self.colors["warning"],
                linestyle="--",
                linewidth=2,
                alpha=0.8,
            )
            ax.axhline(
                orig_valid,
                color=self.colors["warning"],
                linestyle=":",
                linewidth=2,
                alpha=0.8,
            )
        ax.set_title(
            "Final Performance Comparison by Time Window",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_xlabel("Stage")
        ax.set_ylabel("Accuracy")
        ax.set_xticks(x)
        ax.set_xticklabels(stage_names)
        ax.legend()
        ax.grid(True, alpha=0.3)
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.annotate(
                    f"{height:.3f}",
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha="center",
                    va="bottom",
                    fontsize=9,
                    fontweight="bold",
                )
        return stage_names, colors, final_train_acc, final_valid_acc

    def _plot_influence_heatmap(self, ax, stage_results: List[Dict]):
        """ç»˜åˆ¶å½±å“åŠ›åˆ†å¸ƒçƒ­å›¾."""
        all_influences = []
        for stage in stage_results:
            all_influences.extend(np.array(stage["influence_analysis"]["scores"]))
        if not all_influences:
            return
        global_min, global_max = np.min(all_influences), np.max(all_influences)
        influence_matrix = []
        heatmap_labels = []
        for stage in stage_results:
            inf_scores = np.array(stage["influence_analysis"]["scores"])
            heatmap_labels.append(
                f"{stage['stage_name']}\n{stage['time_window']['description']}"
            )
            hist, _ = np.histogram(inf_scores, bins=60, range=(global_min, global_max))
            influence_matrix.append(hist)
        influence_matrix = np.array(influence_matrix)
        im = ax.imshow(
            influence_matrix, aspect="auto", cmap="plasma", interpolation="bilinear"
        )
        ax.set_title(
            "Influence Score Distribution Heatmap Across Time Windows",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_xlabel("Influence Score Bins (Low -> High Influence)")
        ax.set_ylabel("Time Window Stage")
        ax.set_yticks(range(len(heatmap_labels)))
        ax.set_yticklabels(heatmap_labels)
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label("Sample Frequency", rotation=270, labelpad=15)

    def _plot_noise_effectiveness(
        self, ax, stage_results: List[Dict], stage_names: List[str], colors: List[str]
    ):
        """ç»˜åˆ¶å™ªå£°æ£€æµ‹æ•ˆæžœæ•£ç‚¹å›¾."""
        noise_recall = [
            s["pruning_analysis"]["noise_detection"]["noise_recall"]
            for s in stage_results
        ]
        noise_precision = [
            s["pruning_analysis"]["noise_detection"]["noise_precision"]
            for s in stage_results
        ]
        ax.scatter(
            noise_recall,
            noise_precision,
            s=200,
            c=colors,
            alpha=0.8,
            edgecolor="black",
            linewidth=2,
        )
        for recall, precision, name in zip(noise_recall, noise_precision, stage_names):
            ax.annotate(
                name,
                (recall, precision),
                xytext=(8, 8),
                textcoords="offset points",
                fontsize=11,
                fontweight="bold",
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
            )
        ax.set_title(
            "Noise Detection Effectiveness: Recall vs Precision",
            fontsize=14,
            fontweight="bold",
        )
        ax.set_xlabel("Noise Recall")
        ax.set_ylabel("Noise Precision")
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.plot([0, 1], [0, 1], "k--", alpha=0.3, linewidth=2, label="Perfect Balance")
        ax.axhspan(0.8, 1.0, 0.8, 1.0, alpha=0.1, color="green", label="Ideal Region")
        ax.legend()

    def _plot_metrics_table(self, ax, stage_results: List[Dict]):
        """ç»˜åˆ¶é˜¶æ®µæ€§èƒ½æŒ‡æ ‡è¡¨æ ¼."""
        headers = [
            "Stage",
            "Train Acc",
            "Valid Acc",
            "Noise Recall",
            "Noise Prec",
            "Mean Influence",
        ]
        rows = []
        for stage in stage_results:
            final_perf = stage["training_results"]["pruned_history"].get(
                "final_performance", {}
            )
            noise_det = stage["pruning_analysis"]["noise_detection"]
            inf_stats = stage["influence_analysis"]
            rows.append(
                [
                    stage["stage_name"],
                    f"{final_perf.get('train_accuracy', 0):.3f}",
                    f"{final_perf.get('valid_accuracy', 0):.3f}",
                    f"{noise_det['noise_recall']:.2%}",
                    f"{noise_det['noise_precision']:.2%}",
                    f"{inf_stats['mean_influence']:.6f}",
                ]
            )
        table = ax.table(
            cellText=rows,
            colLabels=headers,
            cellLoc="center",
            loc="center",
            colColours=["lightblue"] * len(headers),
        )
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.8)
        ax.set_title("Performance Metrics Summary", fontsize=14, fontweight="bold")
        ax.axis("off")

    def _plot_time_window_diagram(
        self, ax, experiment_config: Dict, stage_results: List[Dict]
    ):
        """ç»˜åˆ¶æ—¶é—´çª—å£åˆ†æ®µç¤ºæ„å›¾."""
        total_epochs = experiment_config.get("epochs", 10)
        time_windows = [(0, 2), (2, 4), (4, 6), (6, 8), (8, None)]
        timeline = np.arange(total_epochs + 1)
        ax.plot(timeline, [0.5] * len(timeline), "k-", linewidth=4, alpha=0.4)
        for i, (start, end) in enumerate(time_windows[: len(stage_results)]):
            end_epoch = total_epochs if end is None else end
            window_range = np.arange(start, end_epoch + 1)
            color = self.colors["stage_colors"][i % len(self.colors["stage_colors"])]
            ax.plot(
                window_range,
                [0.5] * len(window_range),
                color=color,
                linewidth=16,
                alpha=0.8,
                label=f"Stage {i+1}: [{start}, {'T' if end is None else end}]",
            )
            mid_point = (start + end_epoch) / 2
            ax.text(
                mid_point,
                0.7,
                f"S{i+1}",
                ha="center",
                va="center",
                fontweight="bold",
                fontsize=16,
                bbox=dict(
                    boxstyle="circle,pad=0.5",
                    facecolor="white",
                    edgecolor=color,
                    linewidth=3,
                ),
            )
        ax.set_title("Training Time Windows Division", fontsize=14, fontweight="bold")
        ax.set_xlabel("Training Epoch")
        ax.set_xlim(-0.5, total_epochs + 0.5)
        ax.set_ylim(0.2, 0.9)
        ax.set_yticks([])
        ax.legend(loc="upper center", bbox_to_anchor=(0.5, -0.1), ncol=3)
        ax.grid(True, axis="x", alpha=0.3)

    def _plot_experiment_summary(
        self,
        ax,
        experiment_config: Dict,
        data_stats: Dict,
        final_train_acc: List[float],
        final_valid_acc: List[float],
        stage_results: List[Dict],
    ):
        """ç»˜åˆ¶å®žéªŒæ‘˜è¦æ–‡æœ¬å—."""
        noise_recall = [
            s["pruning_analysis"]["noise_detection"]["noise_recall"]
            for s in stage_results
        ]
        noise_precision = [
            s["pruning_analysis"]["noise_detection"]["noise_precision"]
            for s in stage_results
        ]
        best_train_acc = max(final_train_acc) if final_train_acc else 0
        best_valid_acc = max(final_valid_acc) if final_valid_acc else 0
        best_noise_recall = max(noise_recall) if noise_recall else 0
        best_noise_precision = max(noise_precision) if noise_precision else 0
        total_samples = data_stats.get("clean_data_info", {}).get("train_samples", 0)
        noise_count = data_stats.get("noise_info", {}).get("noise_count", 0)
        noise_rate = experiment_config.get("noise_rate", 0)
        summary_text = (
            "MULTI-STAGE EXPERIMENT SUMMARY\n\n"
            f"Dataset Configuration:\n"
            f"â€¢ Dataset: {experiment_config.get('dataset_name', 'N/A').upper()}\n"
            f"â€¢ Total Training Samples: {total_samples:,}\n"
            f"â€¢ Injected Noise Samples: {noise_count:,}\n"
            f"â€¢ Noise Rate: {noise_rate*100:.1f}%\n"
            f"â€¢ Time Window Stages: {experiment_config.get('num_stages', 5)}\n\n"
            f"Performance Highlights:\n"
            f"â€¢ Best Training Accuracy: {best_train_acc:.3f}\n"
            f"â€¢ Best Validation Accuracy: {best_valid_acc:.3f}\n"
            f"â€¢ Best Noise Recall: {best_noise_recall:.1%}\n"
            f"â€¢ Best Noise Precision: {best_noise_precision:.1%}\n\n"
            f"Model Configuration:\n"
            f"â€¢ Architecture: {experiment_config.get('model_name', 'N/A')}\n"
        )
        ax.text(
            0.05,
            0.95,
            summary_text,
            transform=ax.transAxes,
            fontsize=12,
            verticalalignment="top",
            fontfamily="monospace",
            bbox=dict(
                boxstyle="round,pad=0.5", facecolor=self.colors["neutral"], alpha=0.1
            ),
        )
        ax.set_title(
            "Experiment Configuration & Results Summary", fontsize=14, fontweight="bold"
        )
        ax.axis("off")

    def save_experiment_summary(
        self,
        experiment_results: Dict,
        save_name: str = "multi_stage_experiment_summary.json",
    ):
        """
        Save experiment results summary

        Parameters:
        -----------
        experiment_results : Dict
            Complete experiment results
        save_name : str
            Save filename
        """
        save_path = self.save_dir / save_name

        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(experiment_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"Multi-stage experiment summary saved: {save_path}")


def create_multi_stage_visualizer(
    save_dir: str = "./multi_stage_plots",
    figure_size: Tuple[int, int] = (12, 8),
    dpi: int = 150,
) -> MultiStageVisualizer:
    """
    Factory function: Create multi-stage experiment visualizer

    Parameters:
    -----------
    save_dir : str
        Directory to save plots, default "./multi_stage_plots"
    figure_size : Tuple[int, int]
        Figure size, default (12, 8)
    dpi : int
        Figure DPI, default 150

    Returns:
    --------
    MultiStageVisualizer
        Configured multi-stage visualizer
    """
    return MultiStageVisualizer(save_dir=save_dir, figure_size=figure_size, dpi=dpi)


if __name__ == "__main__":
    # Test multi-stage visualizer
    print("ðŸ§ª Testing multi-stage visualizer")

    # Create visualizer
    visualizer = create_multi_stage_visualizer()

    # Generate test data for 5 stages
    np.random.seed(42)

    test_stage_results = []
    for i in range(5):
        # Mock stage results
        n_samples = 1000
        influence_scores = np.random.normal(0.5, 0.2, n_samples)

        # Mock training history
        epochs = 5
        test_history = {
            "train_loss": np.random.exponential(0.5, epochs) + 0.1,
            "train_accuracy": np.random.beta(8, 2, epochs),
            "final_performance": {
                "train_accuracy": np.random.uniform(0.7, 0.9),
                "valid_accuracy": np.random.uniform(0.65, 0.85),
                "train_loss": np.random.uniform(0.1, 0.5),
            },
        }

        stage_result = {
            "stage_num": i + 1,
            "stage_name": f"Stage_{i+1}",
            "time_window": {
                "start_epoch": i * 2,
                "end_epoch": (i + 1) * 2 if i < 4 else None,
                "description": f'[{i*2}, {"T" if i == 4 else (i+1)*2}]',
            },
            "influence_analysis": {
                "scores": influence_scores.tolist(),
                "mean_influence": float(np.mean(influence_scores)),
                "std_influence": float(np.std(influence_scores)),
            },
            "training_results": {"pruned_history": test_history},
            "pruning_analysis": {
                "noise_detection": {
                    "noise_recall": np.random.uniform(0.4, 0.8),
                    "noise_precision": np.random.uniform(0.3, 0.7),
                }
            },
        }

        test_stage_results.append(stage_result)

    print("ðŸŽ¨ Testing multi-stage visualizations...")

    # Test multi-stage influence comparison
    visualizer.plot_multi_stage_influence_comparison(
        test_stage_results, title="Test Multi-Stage Influence Comparison"
    )

    # Test stage performance comparison
    visualizer.plot_stage_performance_comparison(
        test_stage_results, title="Test Stage Performance Comparison"
    )

    # Test influence heatmap
    visualizer.plot_influence_heatmap(
        test_stage_results, title="Test Influence Heatmap"
    )

    # Test time window diagram
    time_windows = [(0, 2), (2, 4), (4, 6), (6, 8), (8, None)]
    visualizer.plot_time_window_diagram(
        time_windows, total_epochs=10, title="Test Time Window Diagram"
    )

    print("âœ… Multi-stage visualizer test complete")

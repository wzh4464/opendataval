#!/usr/bin/env python3
"""
ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°æ¡†æ¶

å®ç°å°†"åªæœ‰å…¨å±€åˆ†æ•°"çš„ä¼°å€¼æ–¹æ³•è½¬æ¢ä¸ºé€epochçš„ç´¯ç§¯å·®åˆ†è¾“å‡ºã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- I(e): å°†ç¬¬eä¸ªepochçš„æ£€æŸ¥ç‚¹å½“ä½œ"æœ€ç»ˆæ¨¡å‹"æ¥è®¡ç®—å…¨å±€å½±å“åŠ›å‘é‡
- Î”I(e) = I(e) - I(e-1): ç¬¬eè½®çš„æ–°å¢è´¡çŒ® (I(-1) = 0å‘é‡)
- è¾“å‡ºCSV: æ¯åˆ—influence_epoch_eå¡«Î”I(e)
- æœ›è¿œé•œæ±‚å’Œ: æ‰€æœ‰åˆ—ç›¸åŠ åº”ç­‰äºæœ€ç»ˆå½±å“åŠ›I(E)

æ”¯æŒæ‰€æœ‰å…¨å±€åˆ†æ•°ä¼°å€¼æ–¹æ³•ï¼šLAVAã€KNNShapleyã€InfluenceFunctionç­‰
"""

from __future__ import annotations

import argparse
import csv
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Type, Union

import numpy as np
import torch
from numpy.random import RandomState
from sklearn.utils import check_random_state

from opendataval.dataloader import DataFetcher
from opendataval.dataval.api import DataEvaluator
from opendataval.dataval.lava import LavaEvaluator
from opendataval.dataval.knnshap import KNNShapley
from opendataval.dataval.influence import InfluenceFunction
from opendataval.model import Model, BertClassifier
from opendataval.util import set_random_seed


class ModelCheckpointManager:
    """ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹æ£€æŸ¥ç‚¹"""

    def __init__(self, base_model: Model, device: torch.device):
        self.base_model = base_model
        self.device = device
        self.checkpoints: Dict[int, Dict[str, Any]] = {}

    def save_checkpoint(self, epoch: int, model_state: Dict[str, Any]):
        """ä¿å­˜æŒ‡å®šepochçš„æ¨¡å‹çŠ¶æ€"""
        # æ·±æ‹·è´æ¨¡å‹çŠ¶æ€åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜
        checkpoint = {k: v.clone().cpu() if isinstance(v, torch.Tensor) else v
                     for k, v in model_state.items()}
        self.checkpoints[epoch] = checkpoint

    def load_checkpoint(self, epoch: int) -> Model:
        """åŠ è½½æŒ‡å®šepochçš„æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if epoch not in self.checkpoints:
            raise ValueError(f"Checkpoint for epoch {epoch} not found")

        # å…‹éš†åŸºç¡€æ¨¡å‹å¹¶åŠ è½½çŠ¶æ€
        model = self.base_model.clone()
        model.load_state_dict(self.checkpoints[epoch])
        model.to(self.device)
        return model

    def has_checkpoint(self, epoch: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‡å®šepochçš„æ£€æŸ¥ç‚¹"""
        return epoch in self.checkpoints

    def available_epochs(self) -> List[int]:
        """è¿”å›æ‰€æœ‰å¯ç”¨çš„epoch"""
        return sorted(self.checkpoints.keys())


class CumulativeDifferentialEvaluator:
    """ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°å™¨"""

    def __init__(
        self,
        evaluator_class: Type[DataEvaluator],
        evaluator_kwargs: Dict[str, Any],
        device: torch.device = torch.device("cpu"),
        random_state: Optional[RandomState] = None,
    ):
        self.evaluator_class = evaluator_class
        self.evaluator_kwargs = evaluator_kwargs
        self.device = device
        self.random_state = check_random_state(random_state)

        # å­˜å‚¨å†å²å½±å“åŠ›è®¡ç®—ç»“æœ
        self.influence_history: Dict[int, np.ndarray] = {}
        self.checkpoint_manager: Optional[ModelCheckpointManager] = None

    def setup_data(
        self,
        x_train: Union[torch.Tensor, List],
        y_train: torch.Tensor,
        x_valid: Union[torch.Tensor, List],
        y_valid: torch.Tensor,
    ):
        """è®¾ç½®è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        self.x_train = x_train
        self.y_train = y_train
        self.x_valid = x_valid
        self.y_valid = y_valid

    def setup_checkpoint_manager(self, base_model: Model):
        """è®¾ç½®æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
        self.checkpoint_manager = ModelCheckpointManager(base_model, self.device)

    def train_with_checkpoints(
        self,
        model: Model,
        epochs: int,
        batch_size: int = 32,
        lr: float = 1e-3,
        save_every: int = 1,
        **train_kwargs
    ) -> Model:
        """è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æ£€æŸ¥ç‚¹"""
        if self.checkpoint_manager is None:
            raise ValueError("Checkpoint manager not initialized")

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {epochs} è½®ï¼Œæ¯ {save_every} è½®ä¿å­˜æ£€æŸ¥ç‚¹")

        # ä¿å­˜åˆå§‹çŠ¶æ€ (epoch -1, å®é™…ä¸Šæ˜¯epoch 0çš„åˆå§‹çŠ¶æ€)
        self.checkpoint_manager.save_checkpoint(-1, model.state_dict())

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        if isinstance(self.x_train, list):  # æ–‡æœ¬æ•°æ®
            x_train_tensor = self.y_train  # å ä½ç¬¦ï¼Œå®é™…ä½¿ç”¨åŸå§‹æ–‡æœ¬
        else:
            x_train_tensor = self.x_train

        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            print(f"  ğŸ“ˆ è®­ç»ƒ Epoch {epoch + 1}/{epochs}")

            # æ‰§è¡Œä¸€è½®è®­ç»ƒ
            if hasattr(model, 'fit_epoch'):
                # å¦‚æœæ¨¡å‹æ”¯æŒå•epochè®­ç»ƒ
                model.fit_epoch(self.x_train, self.y_train,
                               batch_size=batch_size, lr=lr, **train_kwargs)
            else:
                # å¦åˆ™è®­ç»ƒ1ä¸ªepoch
                model.fit(self.x_train, self.y_train,
                         epochs=1, batch_size=batch_size, lr=lr, **train_kwargs)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % save_every == 0:
                self.checkpoint_manager.save_checkpoint(epoch, model.state_dict())
                print(f"    ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch {epoch}")

        print("âœ… è®­ç»ƒå®Œæˆ")
        return model

    def compute_influence_at_epoch(self, epoch: int) -> np.ndarray:
        """è®¡ç®—æŒ‡å®šepochæ£€æŸ¥ç‚¹çš„å½±å“åŠ›åˆ†æ•°"""
        if self.checkpoint_manager is None:
            raise ValueError("Checkpoint manager not initialized")

        if epoch in self.influence_history:
            return self.influence_history[epoch]

        print(f"  ğŸ§® è®¡ç®— epoch {epoch} çš„å½±å“åŠ›...")

        # åŠ è½½æŒ‡å®šepochçš„æ¨¡å‹æ£€æŸ¥ç‚¹
        if epoch == -1:
            # ç‰¹æ®Šæƒ…å†µï¼šåˆå§‹æ¨¡å‹ (æœªè®­ç»ƒ)
            model_at_epoch = self.checkpoint_manager.load_checkpoint(-1)
        else:
            model_at_epoch = self.checkpoint_manager.load_checkpoint(epoch)

        # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
        evaluator = self.evaluator_class(**self.evaluator_kwargs)

        # è®¾ç½®æ•°æ®
        evaluator.input_data(self.x_train, self.y_train, self.x_valid, self.y_valid)

        # ç‰¹æ®Šå¤„ç†ä¸åŒç±»å‹çš„è¯„ä¼°å™¨
        if hasattr(evaluator, 'embedding_model'):
            # ModelLessMixin (LAVA, KNNShapley)
            if isinstance(model_at_epoch, BertClassifier):
                # å¯¹äºBERTæ¨¡å‹ï¼Œéœ€è¦åŒ…è£…ä¸ºåµŒå…¥æ¨¡å‹
                from my_experiments.run_lava_bert import BertEmbeddingWrapper
                embedding_model = BertEmbeddingWrapper(model_at_epoch, mode="pooled")
                evaluator.embedding_model = embedding_model
        elif hasattr(evaluator, 'pred_model'):
            # ModelMixin (InfluenceFunction)
            evaluator.pred_model = model_at_epoch

        # è®­ç»ƒå¹¶è®¡ç®—æ•°æ®ä»·å€¼
        evaluator.train_data_values()
        influence_scores = evaluator.evaluate_data_values()

        # ç¼“å­˜ç»“æœ
        self.influence_history[epoch] = influence_scores
        return influence_scores

    def compute_cumulative_differential(
        self,
        epochs: List[int],
        skip_missing: bool = True
    ) -> Dict[int, np.ndarray]:
        """è®¡ç®—ç´¯ç§¯å·®åˆ†å½±å“åŠ›"""
        print(f"ğŸ“Š è®¡ç®—ç´¯ç§¯å·®åˆ†å½±å“åŠ›: epochs {epochs}")

        # ç¡®ä¿epochsæ’åº
        epochs = sorted(epochs)

        cumulative_diffs = {}
        prev_influence = None

        for epoch in epochs:
            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨
            if not self.checkpoint_manager.has_checkpoint(epoch):
                if skip_missing:
                    print(f"âš ï¸  è·³è¿‡ç¼ºå¤±çš„æ£€æŸ¥ç‚¹: epoch {epoch}")
                    continue
                else:
                    raise ValueError(f"Missing checkpoint for epoch {epoch}")

            # è®¡ç®—å½“å‰epochçš„å½±å“åŠ›
            current_influence = self.compute_influence_at_epoch(epoch)

            # è®¡ç®—å·®åˆ†
            if prev_influence is None:
                # ç¬¬ä¸€ä¸ªepochï¼šÎ”I(e) = I(e) - 0
                diff = current_influence.copy()
            else:
                # åç»­epochï¼šÎ”I(e) = I(e) - I(e-1)
                diff = current_influence - prev_influence

            cumulative_diffs[epoch] = diff
            prev_influence = current_influence.copy()

            print(f"    âœ“ epoch {epoch}: å·®åˆ†ç»Ÿè®¡ mean={diff.mean():.6f}, std={diff.std():.6f}")

        return cumulative_diffs

    def save_to_csv(
        self,
        cumulative_diffs: Dict[int, np.ndarray],
        output_path: Path,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """å°†ç´¯ç§¯å·®åˆ†ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶"""
        print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")

        if not cumulative_diffs:
            raise ValueError("No cumulative differential data to save")

        # è·å–æ•°æ®ç»´åº¦
        sample_diff = next(iter(cumulative_diffs.values()))
        n_samples = len(sample_diff)
        epochs = sorted(cumulative_diffs.keys())

        # å†™å…¥CSV
        with open(output_path, 'w', newline='') as f:
            writer = csv.writer(f)

            # å†™å…¥header
            headers = [f'influence_epoch_{e}' for e in epochs]
            writer.writerow(headers)

            # å†™å…¥æ•°æ® (æŒ‰æ ·æœ¬è¡Œï¼ŒæŒ‰epochåˆ—)
            for i in range(n_samples):
                row = [cumulative_diffs[epoch][i] for epoch in epochs]
                writer.writerow(row)

        # ä¿å­˜å…ƒæ•°æ®
        if metadata:
            metadata_path = output_path.with_suffix('.json')
            metadata_full = {
                'epochs': epochs,
                'n_samples': n_samples,
                'evaluator_class': self.evaluator_class.__name__,
                'evaluator_kwargs': self.evaluator_kwargs,
                **metadata
            }

            with open(metadata_path, 'w') as f:
                json.dump(metadata_full, f, indent=2, default=str)

        print(f"âœ… ä¿å­˜å®Œæˆ: {len(epochs)} ä¸ªepoch, {n_samples} ä¸ªæ ·æœ¬")

    def verify_telescope_sum(
        self,
        cumulative_diffs: Dict[int, np.ndarray],
        final_epoch: int,
        tolerance: float = 1e-6
    ) -> bool:
        """éªŒè¯æœ›è¿œé•œæ±‚å’Œï¼šæ‰€æœ‰å·®åˆ†ç›¸åŠ åº”ç­‰äºæœ€ç»ˆå½±å“åŠ›"""
        print(f"ğŸ” éªŒè¯æœ›è¿œé•œæ±‚å’Œ (tolerance={tolerance})")

        # è®¡ç®—æœ€ç»ˆå½±å“åŠ›
        final_influence = self.compute_influence_at_epoch(final_epoch)

        # ç´¯åŠ æ‰€æœ‰å·®åˆ†
        epochs = sorted(cumulative_diffs.keys())
        summed_diffs = np.zeros_like(final_influence)
        for epoch in epochs:
            summed_diffs += cumulative_diffs[epoch]

        # æ¯”è¾ƒ
        diff = np.abs(final_influence - summed_diffs)
        max_diff = np.max(diff)
        mean_diff = np.mean(diff)

        is_valid = max_diff < tolerance

        print(f"  ğŸ“ æœ€å¤§å·®å¼‚: {max_diff:.10f}")
        print(f"  ğŸ“Š å¹³å‡å·®å¼‚: {mean_diff:.10f}")
        print(f"  {'âœ…' if is_valid else 'âŒ'} æœ›è¿œé•œæ±‚å’Œ{'é€šè¿‡' if is_valid else 'å¤±è´¥'}")

        return is_valid


def create_evaluator_from_config(config: Dict[str, Any]) -> Type[DataEvaluator]:
    """æ ¹æ®é…ç½®åˆ›å»ºè¯„ä¼°å™¨ç±»"""
    evaluator_map = {
        'lava': LavaEvaluator,
        'knnshapley': KNNShapley,
        'influence': InfluenceFunction,
    }

    evaluator_name = config['name'].lower()
    if evaluator_name not in evaluator_map:
        raise ValueError(f"Unknown evaluator: {evaluator_name}")

    return evaluator_map[evaluator_name]


def main():
    """ä¸»å‡½æ•°ï¼šç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°CLI"""
    parser = argparse.ArgumentParser(
        description="ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # æ•°æ®é›†é…ç½®
    parser.add_argument("--dataset", default="imdb",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--train-count", type=int, default=1000,
                       help="è®­ç»ƒæ ·æœ¬æ•°")
    parser.add_argument("--valid-count", type=int, default=200,
                       help="éªŒè¯æ ·æœ¬æ•°")
    parser.add_argument("--test-count", type=int, default=200,
                       help="æµ‹è¯•æ ·æœ¬æ•°")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--model", default="bert",
                       choices=["bert", "mlp", "logistic"],
                       help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--pretrained-model",
                       default="distilbert-base-uncased",
                       help="é¢„è®­ç»ƒæ¨¡å‹åç§°(ä»…BERT)")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--epochs", type=int, default=5,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=16,
                       help="æ‰¹é‡å¤§å°")
    parser.add_argument("--lr", type=float, default=2e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--save-every", type=int, default=1,
                       help="æ¯å‡ è½®ä¿å­˜æ£€æŸ¥ç‚¹")

    # è¯„ä¼°å™¨é…ç½®
    parser.add_argument("--evaluator", default="lava",
                       choices=["lava", "knnshapley", "influence"],
                       help="æ•°æ®ä»·å€¼è¯„ä¼°æ–¹æ³•")
    parser.add_argument("--embedding-mode", default="pooled",
                       choices=["pooled", "logits", "probs"],
                       help="åµŒå…¥æ¨¡å¼(ä»…LAVA)")

    # è¾“å‡ºé…ç½®
    parser.add_argument("--output-dir",
                       default="./results/cumulative_differential",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--output-prefix", default="experiment",
                       help="è¾“å‡ºæ–‡ä»¶å‰ç¼€")

    # å…¶ä»–é…ç½®
    parser.add_argument("--device", default="auto",
                       choices=["auto", "cuda", "mps", "cpu"],
                       help="è®¾å¤‡é€‰æ‹©")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--skip-missing-checkpoints", action="store_true",
                       help="è·³è¿‡ç¼ºå¤±çš„æ£€æŸ¥ç‚¹")

    args = parser.parse_args()

    # è®¾å¤‡é€‰æ‹©
    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)

    print(f"ğŸš€ ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°")
    print(f"  è®¾å¤‡: {device}")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  è¯„ä¼°å™¨: {args.evaluator}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")

    # è®¾ç½®éšæœºç§å­
    set_random_seed(args.seed)

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    fetcher = DataFetcher.setup(
        dataset_name=args.dataset,
        train_count=args.train_count,
        valid_count=args.valid_count,
        test_count=args.test_count,
        random_state=args.seed,
    )
    x_train, y_train, x_valid, y_valid, *_ = fetcher.datapoints

    # åˆ›å»ºæ¨¡å‹
    print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
    if args.model == "bert":
        model = BertClassifier(
            pretrained_model_name=args.pretrained_model,
            num_classes=fetcher.label_dim[0]
        ).to(device)
        # å¤„ç†æ–‡æœ¬æ•°æ®æ ¼å¼
        if hasattr(x_train, 'dataset'):
            x_train = [x_train.dataset[i] for i in x_train.indices]
            x_valid = [x_valid.dataset[i] for i in x_valid.indices]
    else:
        raise NotImplementedError(f"Model {args.model} not implemented yet")

    # åˆ›å»ºç´¯ç§¯å·®åˆ†è¯„ä¼°å™¨
    print("âš™ï¸  åˆ›å»ºç´¯ç§¯å·®åˆ†è¯„ä¼°å™¨...")
    evaluator_kwargs = {}
    if args.evaluator == "lava":
        evaluator_kwargs = {
            "device": device,
            "random_state": args.seed,
        }
    elif args.evaluator == "knnshapley":
        evaluator_kwargs = {
            "random_state": args.seed,
        }
    elif args.evaluator == "influence":
        evaluator_kwargs = {}

    evaluator_class = create_evaluator_from_config({"name": args.evaluator})

    cd_evaluator = CumulativeDifferentialEvaluator(
        evaluator_class=evaluator_class,
        evaluator_kwargs=evaluator_kwargs,
        device=device,
        random_state=args.seed,
    )

    # è®¾ç½®æ•°æ®å’Œæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    cd_evaluator.setup_data(x_train, y_train, x_valid, y_valid)
    cd_evaluator.setup_checkpoint_manager(model)

    # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æ£€æŸ¥ç‚¹
    print("ğŸ‹ï¸  è®­ç»ƒæ¨¡å‹...")
    trained_model = cd_evaluator.train_with_checkpoints(
        model=model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        save_every=args.save_every,
    )

    # è®¡ç®—ç´¯ç§¯å·®åˆ†
    available_epochs = cd_evaluator.checkpoint_manager.available_epochs()
    print(f"ğŸ“Š å¯ç”¨æ£€æŸ¥ç‚¹: {available_epochs}")

    cumulative_diffs = cd_evaluator.compute_cumulative_differential(
        epochs=available_epochs,
        skip_missing=args.skip_missing_checkpoints
    )

    # éªŒè¯æœ›è¿œé•œæ±‚å’Œ
    final_epoch = max(available_epochs)
    cd_evaluator.verify_telescope_sum(cumulative_diffs, final_epoch)

    # ä¿å­˜ç»“æœ
    output_file = output_dir / f"{args.output_prefix}_{args.dataset}_{args.evaluator}_seed{args.seed}.csv"
    metadata = {
        "args": vars(args),
        "device": str(device),
        "final_epoch": final_epoch,
        "available_epochs": available_epochs,
    }

    cd_evaluator.save_to_csv(cumulative_diffs, output_file, metadata)

    print("ğŸ‰ ç´¯ç§¯å·®åˆ†è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
ç°ä»£åŒ–ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°CLI

æ”¯æŒå¤šç§æ¨¡å‹ã€æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•çš„ç»„åˆå®éªŒã€‚
ä½¿ç”¨ uv ç®¡ç†ä¾èµ–ï¼šuv run python -m experiments.run_experiment --help

ç‰¹æ€§ï¼š
- é…ç½®æ–‡ä»¶æ”¯æŒ
- å¤šç§æ¨¡å‹ç±»å‹ (BERT, MLP, LogisticRegression)
- å¤šç§æ•°æ®é›† (IMDB, å…¶ä»–NLP/å›¾åƒ/è¡¨æ ¼æ•°æ®)
- å¤šç§è¯„ä¼°æ–¹æ³• (LAVA, KNNShapley, InfluenceFunction)
- ç°ä»£åŒ–æ—¥å¿—å’Œè¿›åº¦æ˜¾ç¤º
- ç»“æœéªŒè¯å’Œç»Ÿè®¡åˆ†æ
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, Any, Optional

import torch
import yaml
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from experiments.cumulative_differential import CumulativeDifferentialEvaluator, create_evaluator_from_config
from experiments.utils import (
    select_device, set_random_seeds, ModelFactory, DataProcessor,
    BertEmbeddingWrapper, ExperimentLogger, validate_csv_output, compute_statistics
)


def load_config(config_path: Optional[Path] = None) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        "dataset": {
            "name": "imdb",
            "train_count": 1000,
            "valid_count": 200,
            "test_count": 200,
            "add_noise": None
        },
        "model": {
            "type": "bert",
            "pretrained_model": "distilbert-base-uncased",
            "kwargs": {}
        },
        "training": {
            "epochs": 5,
            "batch_size": 16,
            "lr": 2e-5,
            "save_every": 1
        },
        "evaluator": {
            "name": "lava",
            "kwargs": {
                "embedding_mode": "pooled"
            }
        },
        "experiment": {
            "output_dir": "./results/cumulative_differential",
            "output_prefix": "experiment",
            "device": "auto",
            "seed": 42,
            "skip_missing_checkpoints": True,
            "verify_telescope": True
        }
    }

    if config_path and config_path.exists():
        with open(config_path) as f:
            if config_path.suffix.lower() in ['.yaml', '.yml']:
                user_config = yaml.safe_load(f)
            else:  # json
                user_config = json.load(f)

        # åˆå¹¶é…ç½®
        def merge_dict(base: Dict[str, Any], update: Dict[str, Any]) -> Dict[str, Any]:
            for key, value in update.items():
                if isinstance(value, dict) and key in base and isinstance(base[key], dict):
                    base[key] = merge_dict(base[key], value)
                else:
                    base[key] = value
            return base

        default_config = merge_dict(default_config, user_config)

    return default_config


def create_model_with_data_compatibility(
    model_config: Dict[str, Any],
    fetcher_info: Dict[str, Any]
) -> torch.nn.Module:
    """æ ¹æ®æ•°æ®é›†ä¿¡æ¯åˆ›å»ºå…¼å®¹çš„æ¨¡å‹"""
    model_type = model_config["type"]

    if model_type == "bert":
        return ModelFactory.create_model(
            model_type=model_type,
            output_dim=fetcher_info["num_classes"],
            pretrained_model_name=model_config.get("pretrained_model", "distilbert-base-uncased"),
            **model_config.get("kwargs", {})
        )
    else:
        return ModelFactory.create_model(
            model_type=model_type,
            input_dim=fetcher_info["input_dim"],
            output_dim=fetcher_info["num_classes"],
            **model_config.get("kwargs", {})
        )


def run_experiment(config: Dict[str, Any], logger: ExperimentLogger):
    """æ‰§è¡Œå®Œæ•´çš„ç´¯ç§¯å·®åˆ†å®éªŒ"""

    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = select_device(config["experiment"]["device"])
    set_random_seeds(config["experiment"]["seed"])

    logger.log(f"ğŸš€ å¼€å§‹ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°å®éªŒ")
    logger.log(f"è®¾å¤‡: {device}")
    logger.log(f"é…ç½®: {config}")

    # 1. å‡†å¤‡æ•°æ®
    logger.log("ğŸ“‚ å‡†å¤‡æ•°æ®...")
    dataset_config = config["dataset"]
    x_train, y_train, x_valid, y_valid, x_test, y_test, fetcher = DataProcessor.prepare_data(
        dataset_name=dataset_config["name"],
        train_count=dataset_config["train_count"],
        valid_count=dataset_config["valid_count"],
        test_count=dataset_config["test_count"],
        random_state=config["experiment"]["seed"],
        add_noise=dataset_config.get("add_noise")
    )

    # è·å–æ•°æ®ä¿¡æ¯
    fetcher_info = {
        "num_classes": fetcher.label_dim[0],
        "input_dim": getattr(fetcher, 'feature_dim', [None])[0],
        "is_text": dataset_config["name"] in ["imdb"]  # å¯æ‰©å±•
    }
    logger.log(f"æ•°æ®ä¿¡æ¯: {fetcher_info}")

    # å¤„ç†æ–‡æœ¬æ•°æ®
    if fetcher_info["is_text"]:
        x_train = DataProcessor.process_text_data(x_train)
        x_valid = DataProcessor.process_text_data(x_valid)
        y_train = DataProcessor.convert_labels(y_train)
        y_valid = DataProcessor.convert_labels(y_valid)

    # 2. åˆ›å»ºæ¨¡å‹
    logger.log("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
    model = create_model_with_data_compatibility(config["model"], fetcher_info)
    model.to(device)
    logger.log(f"æ¨¡å‹ç±»å‹: {config['model']['type']}")

    # 3. åˆ›å»ºç´¯ç§¯å·®åˆ†è¯„ä¼°å™¨
    logger.log("âš™ï¸ åˆ›å»ºç´¯ç§¯å·®åˆ†è¯„ä¼°å™¨...")
    evaluator_config = config["evaluator"]

    # æ ¹æ®è¯„ä¼°å™¨ç±»å‹è®¾ç½®å‚æ•°
    evaluator_kwargs = {
        "device": device,
        "random_state": config["experiment"]["seed"],
    }

    # æ·»åŠ è¯„ä¼°å™¨ç‰¹å®šå‚æ•°
    evaluator_kwargs.update(evaluator_config.get("kwargs", {}))

    evaluator_class = create_evaluator_from_config(evaluator_config)

    cd_evaluator = CumulativeDifferentialEvaluator(
        evaluator_class=evaluator_class,
        evaluator_kwargs=evaluator_kwargs,
        device=device,
        random_state=config["experiment"]["seed"],
    )

    # è®¾ç½®æ•°æ®å’Œæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    cd_evaluator.setup_data(x_train, y_train, x_valid, y_valid)
    cd_evaluator.setup_checkpoint_manager(model)

    # 4. è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æ£€æŸ¥ç‚¹
    logger.log("ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹...")
    training_config = config["training"]

    with tqdm(total=training_config["epochs"], desc="è®­ç»ƒè¿›åº¦") as pbar:
        class ProgressCallback:
            def __init__(self, pbar, logger):
                self.pbar = pbar
                self.logger = logger

            def on_epoch_end(self, epoch):
                self.pbar.update(1)
                self.logger.log(f"å®Œæˆ epoch {epoch + 1}")

        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†è¿›åº¦å›è°ƒï¼Œå®é™…å®ç°ä¸­éœ€è¦ä¿®æ”¹è®­ç»ƒå‡½æ•°
        trained_model = cd_evaluator.train_with_checkpoints(
            model=model,
            epochs=training_config["epochs"],
            batch_size=training_config["batch_size"],
            lr=training_config["lr"],
            save_every=training_config["save_every"],
        )
        pbar.n = training_config["epochs"]
        pbar.refresh()

    # 5. è®¡ç®—ç´¯ç§¯å·®åˆ†
    logger.log("ğŸ“Š è®¡ç®—ç´¯ç§¯å·®åˆ†å½±å“åŠ›...")
    available_epochs = cd_evaluator.checkpoint_manager.available_epochs()
    logger.log(f"å¯ç”¨æ£€æŸ¥ç‚¹: {available_epochs}")

    with tqdm(total=len(available_epochs), desc="è®¡ç®—å½±å“åŠ›") as pbar:
        cumulative_diffs = cd_evaluator.compute_cumulative_differential(
            epochs=available_epochs,
            skip_missing=config["experiment"]["skip_missing_checkpoints"]
        )
        pbar.n = len(available_epochs)
        pbar.refresh()

    # 6. éªŒè¯æœ›è¿œé•œæ±‚å’Œ
    if config["experiment"]["verify_telescope"]:
        logger.log("ğŸ” éªŒè¯æœ›è¿œé•œæ±‚å’Œ...")
        final_epoch = max(available_epochs)
        is_valid = cd_evaluator.verify_telescope_sum(cumulative_diffs, final_epoch)
        logger.log(f"æœ›è¿œé•œæ±‚å’ŒéªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")

    # 7. ä¿å­˜ç»“æœ
    logger.log("ğŸ’¾ ä¿å­˜ç»“æœ...")
    output_dir = Path(config["experiment"]["output_dir"])
    output_prefix = config["experiment"]["output_prefix"]
    dataset_name = config["dataset"]["name"]
    evaluator_name = config["evaluator"]["name"]
    seed = config["experiment"]["seed"]

    output_file = output_dir / f"{output_prefix}_{dataset_name}_{evaluator_name}_seed{seed}.csv"

    metadata = {
        "config": config,
        "device": str(device),
        "available_epochs": available_epochs,
        "fetcher_info": fetcher_info,
    }

    cd_evaluator.save_to_csv(cumulative_diffs, output_file, metadata)

    # 8. éªŒè¯è¾“å‡ºæ–‡ä»¶
    logger.log("âœ… éªŒè¯è¾“å‡ºæ–‡ä»¶...")
    is_valid_csv = validate_csv_output(output_file, list(cumulative_diffs.keys()))

    # 9. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    logger.log("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
    stats_report = {}
    for epoch, diff_data in cumulative_diffs.items():
        stats_report[f"epoch_{epoch}"] = compute_statistics(diff_data)

    # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
    stats_file = output_dir / f"{output_prefix}_{dataset_name}_{evaluator_name}_seed{seed}_stats.json"
    with open(stats_file, 'w') as f:
        json.dump(stats_report, f, indent=2)

    logger.log(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šä¿å­˜è‡³: {stats_file}")

    # 10. å®éªŒæ€»ç»“
    logger.log("ğŸ‰ å®éªŒå®Œæˆ!")
    logger.log(f"ç»“æœæ–‡ä»¶: {output_file}")
    logger.log(f"ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")
    logger.log(f"å¤„ç†æ ·æœ¬æ•°: {len(next(iter(cumulative_diffs.values())))}")
    logger.log(f"å¤„ç†è½®æ•°: {len(cumulative_diffs)}")

    return {
        "output_file": output_file,
        "stats_file": stats_file,
        "is_valid": is_valid_csv,
        "telescope_valid": is_valid if config["experiment"]["verify_telescope"] else None,
        "stats": stats_report
    }


def main():
    """ä¸»CLIå…¥å£"""
    parser = argparse.ArgumentParser(
        description="ç°ä»£åŒ–ç´¯ç§¯å·®åˆ†æ•°æ®ä»·å€¼è¯„ä¼°CLI",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(
        "--config", "-c", type=Path,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (.json æˆ– .yaml)"
    )

    # åŸºç¡€é…ç½®è¦†ç›–
    parser.add_argument("--dataset", help="æ•°æ®é›†åç§°")
    parser.add_argument("--model", help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--evaluator", help="è¯„ä¼°å™¨ç±»å‹")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--device", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--seed", type=int, help="éšæœºç§å­")
    parser.add_argument("--output-dir", type=Path, help="è¾“å‡ºç›®å½•")

    # å®ç”¨é€‰é¡¹
    parser.add_argument("--dry-run", action="store_true", help="æ˜¾ç¤ºé…ç½®ä½†ä¸è¿è¡Œå®éªŒ")
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹")
    parser.add_argument("--list-evaluators", action="store_true", help="åˆ—å‡ºæ”¯æŒçš„è¯„ä¼°å™¨")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # åˆ—å‡ºå¯ç”¨é€‰é¡¹
    if args.list_models:
        print("æ”¯æŒçš„æ¨¡å‹ç±»å‹:")
        for model in ModelFactory.get_supported_models():
            print(f"  - {model}")
        return

    if args.list_evaluators:
        print("æ”¯æŒçš„è¯„ä¼°å™¨ç±»å‹:")
        evaluators = ["lava", "knnshapley", "influence"]
        for evaluator in evaluators:
            print(f"  - {evaluator}")
        return

    # åŠ è½½é…ç½®
    config = load_config(args.config)

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.dataset:
        config["dataset"]["name"] = args.dataset
    if args.model:
        config["model"]["type"] = args.model
    if args.evaluator:
        config["evaluator"]["name"] = args.evaluator
    if args.epochs:
        config["training"]["epochs"] = args.epochs
    if args.device:
        config["experiment"]["device"] = args.device
    if args.seed:
        config["experiment"]["seed"] = args.seed
    if args.output_dir:
        config["experiment"]["output_dir"] = str(args.output_dir)

    # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ—¥å¿—
    output_dir = Path(config["experiment"]["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)

    logger = ExperimentLogger(output_dir)
    logger.save_config(config)

    # æ˜¾ç¤ºé…ç½®
    if args.verbose or args.dry_run:
        print("ğŸ”§ å®éªŒé…ç½®:")
        print(json.dumps(config, indent=2))
        print()

    if args.dry_run:
        print("ğŸƒâ€â™‚ï¸ æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ï¼Œå®é™…ä¸æ‰§è¡Œå®éªŒ")
        return

    try:
        # è¿è¡Œå®éªŒ
        results = run_experiment(config, logger)

        # ä¿å­˜æ—¥å¿—
        logger.save_logs()

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "="*60)
        print("ğŸŠ å®éªŒå®Œæˆ!")
        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {results['output_file']}")
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {results['stats_file']}")
        print(f"âœ… CSVéªŒè¯: {'é€šè¿‡' if results['is_valid'] else 'å¤±è´¥'}")
        if results['telescope_valid'] is not None:
            print(f"ğŸ”­ æœ›è¿œé•œæ±‚å’Œ: {'é€šè¿‡' if results['telescope_valid'] else 'å¤±è´¥'}")
        print("="*60)

    except Exception as e:
        logger.log(f"âŒ å®éªŒå¤±è´¥: {e}", "ERROR")
        logger.save_logs()
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()